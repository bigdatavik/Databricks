{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "name": "scala", 
            "version": "2.11.8"
        }, 
        "kernelspec": {
            "name": "scala-spark20", 
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "source": "# Overview (SCALA)\n## How to Use SparkSession - A Unified Entry Point in Apache Spark 2.0\n\n\nIn Spark 2.0, SparkSession, is a new entry point that subsumes SparkContext, SQLContext, StreamingContext, and HiveContext. For backward compatibiilty, they are preserved. \nSparkSession has many features and in this notebook some of the more important ones are illustrated. Even though, this notebook is written in Scala, similar functionality and APIs exist in Python and Java.\nIn DSX notebooks and Spark REPL, the SparkSession is created for you, stored in a variable called spark.\n\nThe companion blog post\n\n- http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSession.html\n- https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\n- http://www.slideshare.net/databricks/2016-spark-summit-east-keynote-matei-zaharia\n- http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "![alt text](https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png \"Title\")\nhttps://databricks.com/product/getting-started-guide/quick-start#rdds-datasets-and-dataframes", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### APACHE SPARK: RDD, DATAFRAME OR DATASET?\nhttp://www.agildata.com/apache-spark-rdd-vs-dataframe-vs-dataset/", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# PART 1: Exploring SparkSession\nFor backward compatibility, you can access SparkContext, SQLContext, and SparkConf", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 3, 
                    "data": {
                        "text/plain": "org.apache.spark.sql.SparkSession@8f2c0393"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark", 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## SparkContext as part of SparkSession\nPreserved as part of SparkSession for backward compatibility.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 2, 
                    "data": {
                        "text/plain": "org.apache.spark.SparkContext@4cf97f88"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.sparkContext", 
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## sqlContext as part of SparkSession\nPreserved as part of SparkSession for backward compatibility", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 3, 
                    "data": {
                        "text/plain": "org.apache.spark.sql.SQLContext@168e2b35"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.sqlContext", 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Configuring Spark's runtime configuration parameters", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## SparkConf as part of SparkSession\nThrough spark.conf, You manipulate Spark's runtime configruation parameters. Note that all configuration options set are automatically propagated over to Spark and Hadoop during I/O.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "spark.conf.set(\"spark.notebook.name\", \"SparkSessionSimpleZipExample\")", 
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "SparkSessionSimpleZipExample"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.conf.get(\"spark.notebook.name\")", 
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 6, 
                    "data": {
                        "text/plain": "file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s716-cc0c8609c35e27-396c42860ed9/notebook/work/spark-warehouse/"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.conf.get(\"spark.sql.warehouse.dir\")", 
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Spark config variables set can be accessed via SQL with variable subsitution", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 7, 
                    "data": {
                        "text/plain": "[SparkSessionSimpleZipExample: string, ${spark.sql.warehouse.dir}: string]"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.sql(\"select '${spark.notebook.name}', '${spark.sql.warehouse.dir}'\")", 
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Creating DataFrames and Datasets\nThere are a number of ways to create DataFrames and Datasets using the SparkSession APIs. Once either a DataFrame or Dataset is created, you can manipulate your data. For example, for quick exploration of Datasets, you can use the spark.range", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "import org.apache.spark.sql.functions._", 
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+\n| id|\n+---+\n|  5|\n| 10|\n| 15|\n| 20|\n| 25|\n+---+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "val numDS = spark.range(5, 100, 5)\nnumDS.show(5)", 
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-------+------------------+\n|summary|                id|\n+-------+------------------+\n|  count|                19|\n|   mean|              50.0|\n| stddev|28.136571693556885|\n|    min|                 5|\n|    max|                95|\n+-------+------------------+\n\n"
                }
            ], 
            "source": "numDS.describe().show()", 
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Creating a DataFrame from a collection with SparkSession", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "val langPercentDF = spark.createDataFrame(List((\"Scala\", 35), (\"Python\", 30), (\"R\", 15), (\"Java\", 20)))", 
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "val lpDF = langPercentDF.withColumnRenamed(\"_1\", \"language\").withColumnRenamed(\"_2\", \"percent\")", 
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+-------+\n|language|percent|\n+--------+-------+\n|   Scala|     35|\n|  Python|     30|\n|    Java|     20|\n|       R|     15|\n+--------+-------+\n\n"
                }
            ], 
            "source": "lpDF.orderBy(desc(\"percent\")).show()", 
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# PART 2: Exploring Zip codes data using SparkSession and Dataset APIs.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Next, we going to exlore some zip code data fetched from MongoDB", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 12, 
                    "data": {
                        "text/plain": "Name: Unknown Error\nMessage: <console>:22: error: stable identifier required, but this.$line7$read.spark.implicits found.\n       import spark.implicits._\n                    ^\nStackTrace: "
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "--2017-02-09 10:27:49--  http://media.mongodb.org/zips.json\nResolving media.mongodb.org (media.mongodb.org)... 52.85.113.82, 52.85.113.131, 52.85.113.150, ...\nConnecting to media.mongodb.org (media.mongodb.org)|52.85.113.82|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3182409 (3.0M) [application/json]\nSaving to: \u2018zips.json.1\u2019\n\n     0K .......... .......... .......... .......... ..........  1%  154M 0s\n    50K .......... .......... .......... .......... ..........  3%  450M 0s\n   100K .......... .......... .......... .......... ..........  4%  407M 0s\n   150K .......... .......... .......... .......... ..........  6%  461M 0s\n   200K .......... .......... .......... .......... ..........  8% 1.13M 1s\n   250K .......... .......... .......... .......... ..........  9% 2.22M 1s\n   300K .......... .......... .......... .......... .......... 11% 2.24M 1s\n   350K .......... .......... .......... .......... .......... 12% 2.24M 1s\n   400K .......... .......... .......... .......... .......... 14% 72.4M 1s\n   450K .......... .......... .......... .......... .......... 16% 2.24M 1s\n   500K .......... .......... .......... .......... .......... 17% 61.9M 1s\n   550K .......... .......... .......... .......... .......... 19% 2.29M 1s\n   600K .......... .......... .......... .......... .......... 20% 78.8M 1s\n   650K .......... .......... .......... .......... .......... 22% 2.29M 1s\n   700K .......... .......... .......... .......... .......... 24% 67.7M 1s\n   750K .......... .......... .......... .......... .......... 25% 1.36M 1s\n   800K .......... .......... .......... .......... .......... 27%  447M 1s\n   850K .......... .......... .......... .......... .......... 28%  461M 1s\n   900K .......... .......... .......... .......... .......... 30% 2.23M 1s\n   950K .......... .......... .......... .......... .......... 32%  113M 0s\n  1000K .......... .......... .......... .......... .......... 33%  117M 0s\n  1050K .......... .......... .......... .......... .......... 35% 2.30M 0s\n  1100K .......... .......... .......... .......... .......... 37% 87.1M 0s\n  1150K .......... .......... .......... .......... .......... 38% 2.30M 0s\n  1200K .......... .......... .......... .......... .......... 40%  112M 0s\n  1250K .......... .......... .......... .......... .......... 41% 82.3M 0s\n  1300K .......... .......... .......... .......... .......... 43% 2.30M 0s\n  1350K .......... .......... .......... .......... .......... 45%  110M 0s\n  1400K .......... .......... .......... .......... .......... 46%  106M 0s\n  1450K .......... .......... .......... .......... .......... 48% 2.35M 0s\n  1500K .......... .......... .......... .......... .......... 49% 65.6M 0s\n  1550K .......... .......... .......... .......... .......... 51%  131M 0s\n  1600K .......... .......... .......... .......... .......... 53%  103M 0s\n  1650K .......... .......... .......... .......... .......... 54% 2.39M 0s\n  1700K .......... .......... .......... .......... .......... 56%  115M 0s\n  1750K .......... .......... .......... .......... .......... 57% 41.3M 0s\n  1800K .......... .......... .......... .......... .......... 59%  264M 0s\n  1850K .......... .......... .......... .......... .......... 61% 2.41M 0s\n  1900K .......... .......... .......... .......... .......... 62% 43.1M 0s\n  1950K .......... .......... .......... .......... .......... 64%  119M 0s\n  2000K .......... .......... .......... .......... .......... 65% 45.8M 0s\n  2050K .......... .......... .......... .......... .......... 67%  388M 0s\n  2100K .......... .......... .......... .......... .......... 69% 2.54M 0s\n  2150K .......... .......... .......... .......... .......... 70% 34.2M 0s\n  2200K .......... .......... .......... .......... .......... 72%  116M 0s\n  2250K .......... .......... .......... .......... .......... 74%  104M 0s\n  2300K .......... .......... .......... .......... .......... 75%  103M 0s\n  2350K .......... .......... .......... .......... .......... 77% 2.56M 0s\n  2400K .......... .......... .......... .......... .......... 78%  113M 0s\n  2450K .......... .......... .......... .......... .......... 80% 32.5M 0s\n  2500K .......... .......... .......... .......... .......... 82% 94.3M 0s\n  2550K .......... .......... .......... .......... .......... 83% 83.9M 0s\n  2600K .......... .......... .......... .......... .......... 85%  102M 0s\n  2650K .......... .......... .......... .......... .......... 86%  126M 0s\n  2700K .......... .......... .......... .......... .......... 88% 2.63M 0s\n  2750K .......... .......... .......... .......... .......... 90% 31.3M 0s\n  2800K .......... .......... .......... .......... .......... 91%  106M 0s\n  2850K .......... .......... .......... .......... .......... 93% 89.5M 0s\n  2900K .......... .......... .......... .......... .......... 94%  102M 0s\n  2950K .......... .......... .......... .......... .......... 96%  101M 0s\n  3000K .......... .......... .......... .......... .......... 98% 2.70M 0s\n  3050K .......... .......... .......... .......... .......... 99% 28.6M 0s\n  3100K .......                                               100%  101M=0.5s\n\n2017-02-09 10:27:50 (6.65 MB/s) - \u2018zips.json.1\u2019 saved [3182409/3182409]\n\n"
                }
            ], 
            "source": "import sys.process._\n\"wget http://media.mongodb.org/zips.json\" !", 
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "##### The above command runs on your cluster's single node, fetches the zip code file from the specified URL, unzips in the directory below", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 1, 
                    "data": {
                        "text/plain": "Name: Compile Error\nMessage: <console>:18: error: value ! is not a member of String\n       \"pwd\" !\n             ^\nStackTrace: "
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "\"pwd\" !", 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "spark-warehouse\nzips.json\nzips.json.1\n"
                }
            ], 
            "source": "\"ls\" !", 
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Reading the JSON file with SparkSession\n### Read the JSON file, infer the schema and convert it into a Dataset dictated by the case class Zips", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "import org.apache.spark.sql.SparkSession\nval spark = (SparkSession.\n    builder().\n    getOrCreate())\n// For implicit conversions like converting RDDs to DataFrames\n// This import is needed to use the $-notation\nimport spark.implicits._", 
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\nimport org.apache.spark.sql.Encoder\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n// A case class for zips data\ncase class Zips(zip:String, city:String, loc:Array[Double], pop:Long, state:String)", 
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+--------------------+-----+-----+\n|  zip|           city|                 loc|  pop|state|\n+-----+---------------+--------------------+-----+-----+\n|01001|         AGAWAM|[-72.622739, 42.0...|15338|   MA|\n|01002|        CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n|01005|          BARRE|[-72.108354, 42.4...| 4546|   MA|\n|01007|    BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n|01008|      BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n|01010|      BRIMFIELD|[-72.188455, 42.1...| 3706|   MA|\n|01011|        CHESTER|[-72.988761, 42.2...| 1688|   MA|\n|01012|   CHESTERFIELD|[-72.833309, 42.3...|  177|   MA|\n|01013|       CHICOPEE|[-72.607962, 42.1...|23396|   MA|\n|01020|       CHICOPEE|[-72.576142, 42.1...|31495|   MA|\n|01022|   WESTOVER AFB|[-72.558657, 42.1...| 1764|   MA|\n|01026|     CUMMINGTON|[-72.905767, 42.4...| 1484|   MA|\n|01027|      MOUNT TOM|[-72.679921, 42.2...|16864|   MA|\n|01028|EAST LONGMEADOW|[-72.505565, 42.0...|13367|   MA|\n|01030|  FEEDING HILLS|[-72.675077, 42.0...|11985|   MA|\n|01031|   GILBERTVILLE|[-72.198585, 42.3...| 2385|   MA|\n|01032|         GOSHEN|[-72.844092, 42.4...|  122|   MA|\n|01033|         GRANBY|[-72.520001, 42.2...| 5526|   MA|\n|01034|        TOLLAND|[-72.908793, 42.0...| 1652|   MA|\n|01035|         HADLEY|[-72.571499, 42.3...| 4231|   MA|\n+-----+---------------+--------------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "val zipDF = spark.read.json(\"zips.json\").withColumnRenamed(\"_id\",\"zip\")\n//rename the _id to zip for readability\n//convert to a dataset using the case class\n//val zipDS = zipDF.withColumnRenamed(\"_id\",\"zip\").as[Zips]\nval zipDS = zipDF.as[Zips]\n// since we will be quering this dataset often let's cache it\nzipDS.cache()\n//display(zipDS)\nzipDS.show()", 
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Zips(01001,AGAWAM,[D@57290dce,15338,MA)\nZips(01002,CUSHMAN,[D@fca59a77,36963,MA)\nZips(01005,BARRE,[D@ffcc0eca,4546,MA)\nZips(01007,BELCHERTOWN,[D@1172d4a5,10579,MA)\nZips(01008,BLANDFORD,[D@37d80f38,1240,MA)\n"
                }
            ], 
            "source": "zipDS.take(5).foreach(println)", 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Q1: Can you display states, zips, cities with population greater than 40000, in descending order", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+-----+------+\n|state|           city|  zip|   pop|\n+-----+---------------+-----+------+\n|   IL|        CHICAGO|60623|112047|\n|   NY|       BROOKLYN|11226|111396|\n|   NY|       NEW YORK|10021|106564|\n|   NY|       NEW YORK|10025|100027|\n|   CA|   BELL GARDENS|90201| 99568|\n|   IL|        CHICAGO|60617| 98612|\n|   CA|    LOS ANGELES|90011| 96074|\n|   IL|        CHICAGO|60647| 95971|\n|   IL|        CHICAGO|60628| 94317|\n|   CA|        NORWALK|90650| 94188|\n|   IL|        CHICAGO|60620| 92005|\n|   IL|        CHICAGO|60629| 91814|\n|   IL|        CHICAGO|60609| 89762|\n|   IL|        CHICAGO|60618| 88377|\n|   NY|JACKSON HEIGHTS|11373| 88241|\n|   CA|         ARLETA|91331| 88114|\n|   NY|       BROOKLYN|11212| 87079|\n|   CA|     SOUTH GATE|90280| 87026|\n|   NY|      RIDGEWOOD|11385| 85732|\n|   NY|          BRONX|10467| 85710|\n+-----+---------------+-----+------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "//display(zipDS.select(\"state\", \"city\", \"zip\", \"pop\").filter(\"pop > 40000\").orderBy(desc(\"pop\")))\nzipDS.select(\"state\", \"city\", \"zip\", \"pop\").filter(\"pop > 40000\").orderBy(desc(\"pop\")).show()", 
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Q2: Which cities and zips in the state of california are most populous?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+-----+-----+\n|            city|  zip|  pop|\n+----------------+-----+-----+\n|    BELL GARDENS|90201|99568|\n|     LOS ANGELES|90011|96074|\n|         NORWALK|90650|94188|\n|          ARLETA|91331|88114|\n|      SOUTH GATE|90280|87026|\n|     LOS ANGELES|90044|83958|\n|         FONTANA|92335|81255|\n|      HOLLY PARK|90250|78511|\n|     WESTMINSTER|92683|77965|\n|       SANTA ANA|92704|77151|\n|        INDUSTRY|91744|77114|\n|COAST GUARD ISLA|94501|76110|\n|          RIALTO|92376|75341|\n|     LOS ANGELES|90026|74751|\n|      LONG BEACH|90805|74011|\n| HUNTINGTON PARK|90255|72139|\n|   MORENO VALLEY|92553|71314|\n|LAKE LOS ANGELES|93550|71024|\n|   SAN FRANCISCO|94110|70770|\n|       IRWINDALE|91706|69464|\n+----------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "//display(zipDS.select(\"city\", \"zip\", \"pop\").filter('state === \"CA\").orderBy(desc(\"pop\")))\nzipDS.select(\"city\", \"zip\", \"pop\").filter('state === \"CA\").orderBy(desc(\"pop\")).show()", 
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Q3: Can you sum up the population of all the states and order them in descending order?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+--------+\n|state|sum(pop)|\n+-----+--------+\n|   CA|29754890|\n|   NY|17990402|\n|   TX|16984601|\n|   FL|12686644|\n|   PA|11881643|\n|   IL|11427576|\n|   OH|10846517|\n|   MI| 9295297|\n|   NJ| 7730188|\n|   NC| 6628637|\n|   GA| 6478216|\n|   VA| 6181479|\n|   MA| 6016425|\n|   IN| 5544136|\n|   MO| 5110648|\n|   WI| 4891769|\n|   TN| 4876457|\n|   WA| 4866692|\n|   MD| 4781379|\n|   MN| 4372982|\n+-----+--------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "//display(zipDS.select(\"state\", \"pop\").groupBy(\"state\").sum(\"pop\").orderBy(desc(\"sum(pop)\")))\nzipDS.select(\"state\", \"pop\").groupBy(\"state\").sum(\"pop\").orderBy(desc(\"sum(pop)\")).show()", 
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# PART 3: Creating Hive Table, registering a UDF, and querying it using SparkSession and Spark SQL APIs", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### drop the table if one exists", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 70, 
                    "data": {
                        "text/plain": "[]"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.sql(\"DROP TABLE IF EXISTS hive_zips_table\")", 
            "execution_count": 70, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "import sys.process._", 
            "execution_count": 71, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### Just ensure we don't have any lingering files in the directory because of eventual consistency.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "ls: cannot access /gpfs/global_fs01/sym_shared/YPProdSpark/user/s716-cc0c8609c35e27-396c42860ed9/notebook/work/spark-warehouse/hive_zips_table: No such file or directory\n"
                }
            ], 
            "source": "// \"ls /gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/hive_zips_table\" !\n\"ls /gpfs/global_fs01/sym_shared/YPProdSpark/user/s716-cc0c8609c35e27-396c42860ed9/notebook/work/spark-warehouse/hive_zips_table\" !", 
            "execution_count": 72, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "// \"rm -rf /gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/hive_zips_table\" !\n// \"rm -rf /gpfs/global_fs01/sym_shared/YPProdSpark/user/s716-cc0c8609c35e27-396c42860ed9/notebook/work/spark-warehouse/hive_zips_table1\" !\n\n\"rm -rf /gpfs/global_fs01/sym_shared/YPProdSpark/user/s716-cc0c8609c35e27-396c42860ed9/notebook/work/spark-warehouse/hive_zips_table\" !", 
            "execution_count": 73, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "zipDS.write.saveAsTable(\"hive_zips_table\")", 
            "execution_count": 74, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Working and Accessing Catalog metadata", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-------+----------------+-----------------------------------------------------------------------------------------------------------------+\n|name   |description     |locationUri                                                                                                      |\n+-------+----------------+-----------------------------------------------------------------------------------------------------------------+\n|default|default database|file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s716-cc0c8609c35e27-396c42860ed9/notebook/work/spark-warehouse|\n+-------+----------------+-----------------------------------------------------------------------------------------------------------------+\n\n"
                }
            ], 
            "source": "//display(spark.catalog.listDatabases)\nspark.catalog.listDatabases.show(false)", 
            "execution_count": 75, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------------+--------+-----------+---------+-----------+\n|name           |database|description|tableType|isTemporary|\n+---------------+--------+-----------+---------+-----------+\n|hive_zips_table|default |null       |MANAGED  |false      |\n+---------------+--------+-----------+---------+-----------+\n\n"
                }
            ], 
            "source": "//display(spark.catalog.listTables)\nspark.catalog.listTables.show(false)", 
            "execution_count": 76, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Cache table using SparkSession API", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "spark.catalog.cacheTable(\"hive_zips_table\")", 
            "execution_count": 78, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## Q1: Can you query the Hive table with the Spark SQL query indentical to the one above Q1?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+-----+------+\n|state|           city|  zip|   pop|\n+-----+---------------+-----+------+\n|   IL|        CHICAGO|60623|112047|\n|   NY|       BROOKLYN|11226|111396|\n|   NY|       NEW YORK|10021|106564|\n|   NY|       NEW YORK|10025|100027|\n|   CA|   BELL GARDENS|90201| 99568|\n|   IL|        CHICAGO|60617| 98612|\n|   CA|    LOS ANGELES|90011| 96074|\n|   IL|        CHICAGO|60647| 95971|\n|   IL|        CHICAGO|60628| 94317|\n|   CA|        NORWALK|90650| 94188|\n|   IL|        CHICAGO|60620| 92005|\n|   IL|        CHICAGO|60629| 91814|\n|   IL|        CHICAGO|60609| 89762|\n|   IL|        CHICAGO|60618| 88377|\n|   NY|JACKSON HEIGHTS|11373| 88241|\n|   CA|         ARLETA|91331| 88114|\n|   NY|       BROOKLYN|11212| 87079|\n|   CA|     SOUTH GATE|90280| 87026|\n|   NY|      RIDGEWOOD|11385| 85732|\n|   NY|          BRONX|10467| 85710|\n+-----+---------------+-----+------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "//display(spark.sql(\"SELECT state, city, zip, pop FROM hive_zips_table WHERE pop > 40000 ORDER BY pop DESC\"))\nspark.sql(\"SELECT state, city, zip, pop FROM hive_zips_table WHERE pop > 40000 ORDER BY pop DESC\").show()", 
            "execution_count": 79, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Q2: Find the populus cities in Calfornia with total number of zips using the hive table?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------+--------+----------------+\n|count(zip)|sum(pop)|            city|\n+----------+--------+----------------+\n|        56| 2102295|     LOS ANGELES|\n|        34| 1049298|       SAN DIEGO|\n|        29|  816653|        SAN JOSE|\n|        26|  723993|   SAN FRANCISCO|\n|        28|  628279|      SACRAMENTO|\n|        12|  347905|          FRESNO|\n|        12|  314487|         OAKLAND|\n|         8|  299651|      LONG BEACH|\n|         7|  272327|         ANAHEIM|\n|         8|  271347|     BAKERSFIELD|\n|        11|  267258|        STOCKTON|\n|         7|  253478|       RIVERSIDE|\n|         4|  234472|       SANTA ANA|\n|         5|  216459|         MODESTO|\n|         4|  183542|HUNTINGTON BEACH|\n|         7|  177552|  SAN BERNARDINO|\n|         4|  173374|         FREMONT|\n|         8|  163666|        GLENDALE|\n|         6|  158398|      SANTA ROSA|\n|         6|  158183|        TORRANCE|\n+----------+--------+----------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "// display(spark.sql(\"SELECT COUNT(zip), SUM(pop), city FROM hive_zips_table WHERE state = 'CA' GROUP BY city ORDER BY SUM(pop) DESC\"))\nspark.sql(\"SELECT COUNT(zip), SUM(pop), city FROM hive_zips_table WHERE state = 'CA' GROUP BY city ORDER BY SUM(pop) DESC\").show()", 
            "execution_count": 80, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Registring a UDF with SparkSession", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Q4: Can you register a simple UDF with SparkSession that converts zip into long (currently it's a string)?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+-------------+-------+\n|col_name|    data_type|comment|\n+--------+-------------+-------+\n|     zip|       string|   null|\n|    city|       string|   null|\n|     loc|array<double>|   null|\n|     pop|       bigint|   null|\n|   state|       string|   null|\n+--------+-------------+-------+\n\n"
                }
            ], 
            "source": "spark.sql(\"describe hive_zips_table\").show()", 
            "execution_count": 81, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 82, 
                    "data": {
                        "text/plain": "UserDefinedFunction(<function1>,LongType,Some(List(StringType)))"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.udf.register(\"zipToLong\", (z:String) => z.toLong)", 
            "execution_count": 82, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----------+-----------+\n|       city|zip_to_long|\n+-----------+-----------+\n|  KETCHIKAN|      99950|\n|   WRANGELL|      99929|\n|POINT BAKER|      99927|\n| METLAKATLA|      99926|\n|    KLAWOCK|      99925|\n|      HYDER|      99923|\n|   HYDABURG|      99922|\n|      CRAIG|      99921|\n| THORNE BAY|      99919|\n|  KETCHIKAN|      99901|\n|    SKAGWAY|      99840|\n|      SITKA|      99835|\n| PETERSBURG|      99833|\n|     HOONAH|      99829|\n|     HAINES|      99827|\n|   GUSTAVUS|      99826|\n|    DOUGLAS|      99824|\n|     ANGOON|      99820|\n|     JUNEAU|      99801|\n|    NUIQSUT|      99789|\n+-----------+-----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "spark.sql(\"SELECT city, zipToLong(zip) as zip_to_long FROM hive_zips_table ORDER BY zip_to_long DESC\").show()", 
            "execution_count": 83, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Register another UDF that calculates the strlen of cities", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 84, 
                    "data": {
                        "text/plain": "UserDefinedFunction(<function1>,IntegerType,Some(List(StringType)))"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.udf.register(\"cityLength\", (c:String) => c.length())", 
            "execution_count": 84, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Using catalog data, get the list of your registered UDFs", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "val udfs = spark.catalog.listFunctions()", 
            "execution_count": 85, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------+--------+\n|      name|database|\n+----------+--------+\n|citylength|    null|\n| ziptolong|    null|\n+----------+--------+\n\n"
                }
            ], 
            "source": "udfs.filter('name === \"cityLength\".toLowerCase || 'name === \"zipToLong\".toLowerCase).select(\"name\", \"database\").show()", 
            "execution_count": 86, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+-----------+\n|            city|city_length|\n+----------------+-----------+\n|CHEBEAGUE ISLAND|         16|\n|MONTGOMERY CENTE|         16|\n|CUMBERLAND CENTE|         16|\n|WEST BRIDGEWATER|         16|\n|OLD ORCHARD BEAC|         16|\n|WEST SPRINGFIELD|         16|\n|CUMBERLAND FORES|         16|\n|GREAT BARRINGTON|         16|\n|NORTH WHITEFIELD|         16|\n|NORTH CHELMSFORD|         16|\n|EAST MILLINOCKET|         16|\n|NEWTON UPPER FAL|         16|\n|GREENVILLE JUNCT|         16|\n|GILMANTON IRON W|         16|\n|LITTLE DEER ISLE|         16|\n|WOOD RIVER JUNCT|         16|\n|SOUTH GOULDSBORO|         16|\n|CENTER BARNSTEAD|         16|\n|SOUTHWEST HARBOR|         16|\n|WEST CHESTERFIEL|         16|\n+----------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "spark.sql(\"SELECT city, cityLength(city) as city_length FROM hive_zips_table ORDER BY city_length DESC\").show()", 
            "execution_count": 87, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Q5: Can you compose the same query as Q2 using Datasets APIs?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+--------+\n|            city|sum(pop)|\n+----------------+--------+\n|     LOS ANGELES| 2102295|\n|       SAN DIEGO| 1049298|\n|        SAN JOSE|  816653|\n|   SAN FRANCISCO|  723993|\n|      SACRAMENTO|  628279|\n|          FRESNO|  347905|\n|         OAKLAND|  314487|\n|      LONG BEACH|  299651|\n|         ANAHEIM|  272327|\n|     BAKERSFIELD|  271347|\n|        STOCKTON|  267258|\n|       RIVERSIDE|  253478|\n|       SANTA ANA|  234472|\n|         MODESTO|  216459|\n|HUNTINGTON BEACH|  183542|\n|  SAN BERNARDINO|  177552|\n|         FREMONT|  173374|\n|        GLENDALE|  163666|\n|      SANTA ROSA|  158398|\n|        TORRANCE|  158183|\n+----------------+--------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "(zipDS.filter('state === \"CA\")\n  .select(\"zip\", \"pop\", \"city\")\n  .groupBy(\"city\")\n  .sum()\n  .orderBy(desc(\"sum(pop)\")).show())", 
            "execution_count": 88, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+----------+\n|            city|population|\n+----------------+----------+\n|     LOS ANGELES|   2102295|\n|       SAN DIEGO|   1049298|\n|        SAN JOSE|    816653|\n|   SAN FRANCISCO|    723993|\n|      SACRAMENTO|    628279|\n|          FRESNO|    347905|\n|         OAKLAND|    314487|\n|      LONG BEACH|    299651|\n|         ANAHEIM|    272327|\n|     BAKERSFIELD|    271347|\n|        STOCKTON|    267258|\n|       RIVERSIDE|    253478|\n|       SANTA ANA|    234472|\n|         MODESTO|    216459|\n|HUNTINGTON BEACH|    183542|\n|  SAN BERNARDINO|    177552|\n|         FREMONT|    173374|\n|        GLENDALE|    163666|\n|      SANTA ROSA|    158398|\n|        TORRANCE|    158183|\n+----------------+----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "(zipDS.filter('state === \"CA\")\n  .select(\"zip\", \"pop\", \"city\")\n  .groupBy(\"city\")\n  .agg(sum(\"pop\").alias(\"population\"))\n  .orderBy(desc(\"population\"))).show()", 
            "execution_count": 92, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+----------+---+\n|            city|population|cnt|\n+----------------+----------+---+\n|     LOS ANGELES|   2102295| 56|\n|       SAN DIEGO|   1049298| 34|\n|        SAN JOSE|    816653| 29|\n|   SAN FRANCISCO|    723993| 26|\n|      SACRAMENTO|    628279| 28|\n|          FRESNO|    347905| 12|\n|         OAKLAND|    314487| 12|\n|      LONG BEACH|    299651|  8|\n|         ANAHEIM|    272327|  7|\n|     BAKERSFIELD|    271347|  8|\n|        STOCKTON|    267258| 11|\n|       RIVERSIDE|    253478|  7|\n|       SANTA ANA|    234472|  4|\n|         MODESTO|    216459|  5|\n|HUNTINGTON BEACH|    183542|  4|\n|  SAN BERNARDINO|    177552|  7|\n|         FREMONT|    173374|  4|\n|        GLENDALE|    163666|  8|\n|      SANTA ROSA|    158398|  6|\n|        TORRANCE|    158183|  6|\n+----------------+----------+---+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "(zipDS.filter('state === \"CA\")\n  .select(\"zip\", \"pop\", \"city\")\n  .groupBy(\"city\")\n  .agg(sum(\"pop\").alias(\"population\"), count(\"*\").alias(\"cnt\"))\n  .orderBy(desc(\"population\"))).show()", 
            "execution_count": 93, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+----------+---+\n|            city|population|zip|\n+----------------+----------+---+\n|     LOS ANGELES|   2102295| 56|\n|       SAN DIEGO|   1049298| 34|\n|        SAN JOSE|    816653| 29|\n|   SAN FRANCISCO|    723993| 26|\n|      SACRAMENTO|    628279| 28|\n|          FRESNO|    347905| 12|\n|         OAKLAND|    314487| 12|\n|      LONG BEACH|    299651|  8|\n|         ANAHEIM|    272327|  7|\n|     BAKERSFIELD|    271347|  8|\n|        STOCKTON|    267258| 11|\n|       RIVERSIDE|    253478|  7|\n|       SANTA ANA|    234472|  4|\n|         MODESTO|    216459|  5|\n|HUNTINGTON BEACH|    183542|  4|\n|  SAN BERNARDINO|    177552|  7|\n|         FREMONT|    173374|  4|\n|        GLENDALE|    163666|  8|\n|      SANTA ROSA|    158398|  6|\n|        TORRANCE|    158183|  6|\n+----------------+----------+---+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "(zipDS.filter('state === \"CA\")\n  .select(\"zip\", \"pop\", \"city\")\n  .groupBy(\"city\")\n  .agg(sum(\"pop\").alias(\"population\"), count(\"zip\").alias(\"zip\"))\n  .orderBy(desc(\"population\"))).show()", 
            "execution_count": 94, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Parquet - Bluemix Object Storage", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Save as parquet file", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 96, 
                    "data": {
                        "text/plain": "Name: org.apache.spark.sql.AnalysisException\nMessage: path swift://Databricks.keystone/zipDS.parquet already exists.;\nStackTrace: org.apache.spark.sql.AnalysisException: path swift://Databricks.keystone/zipDS.parquet already exists.;\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:88)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:525)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:488)"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "zipDS.write.parquet(\"swift://Databricks.\" + name + \"/zipDS.parquet\")", 
            "execution_count": 96, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Read parquet file from Object Storage", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "val zipDSparquet = spark.read.parquet(\"swift://Databricks.\" + name + \"/zipDS.parquet\")", 
            "execution_count": 97, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+--------------------+-----+-----+\n|  zip|           city|                 loc|  pop|state|\n+-----+---------------+--------------------+-----+-----+\n|01001|         AGAWAM|[-72.622739, 42.0...|15338|   MA|\n|01002|        CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n|01005|          BARRE|[-72.108354, 42.4...| 4546|   MA|\n|01007|    BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n|01008|      BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n|01010|      BRIMFIELD|[-72.188455, 42.1...| 3706|   MA|\n|01011|        CHESTER|[-72.988761, 42.2...| 1688|   MA|\n|01012|   CHESTERFIELD|[-72.833309, 42.3...|  177|   MA|\n|01013|       CHICOPEE|[-72.607962, 42.1...|23396|   MA|\n|01020|       CHICOPEE|[-72.576142, 42.1...|31495|   MA|\n|01022|   WESTOVER AFB|[-72.558657, 42.1...| 1764|   MA|\n|01026|     CUMMINGTON|[-72.905767, 42.4...| 1484|   MA|\n|01027|      MOUNT TOM|[-72.679921, 42.2...|16864|   MA|\n|01028|EAST LONGMEADOW|[-72.505565, 42.0...|13367|   MA|\n|01030|  FEEDING HILLS|[-72.675077, 42.0...|11985|   MA|\n|01031|   GILBERTVILLE|[-72.198585, 42.3...| 2385|   MA|\n|01032|         GOSHEN|[-72.844092, 42.4...|  122|   MA|\n|01033|         GRANBY|[-72.520001, 42.2...| 5526|   MA|\n|01034|        TOLLAND|[-72.908793, 42.0...| 1652|   MA|\n|01035|         HADLEY|[-72.571499, 42.3...| 4231|   MA|\n+-----+---------------+--------------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "zipDSparquet.show()", 
            "execution_count": 98, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "zipDSparquet.registerTempTable(\"zipDSparquet\")", 
            "execution_count": 99, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+--------------------+-----+-----+\n|  zip|           city|                 loc|  pop|state|\n+-----+---------------+--------------------+-----+-----+\n|01001|         AGAWAM|[-72.622739, 42.0...|15338|   MA|\n|01002|        CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n|01005|          BARRE|[-72.108354, 42.4...| 4546|   MA|\n|01007|    BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n|01008|      BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n|01010|      BRIMFIELD|[-72.188455, 42.1...| 3706|   MA|\n|01011|        CHESTER|[-72.988761, 42.2...| 1688|   MA|\n|01012|   CHESTERFIELD|[-72.833309, 42.3...|  177|   MA|\n|01013|       CHICOPEE|[-72.607962, 42.1...|23396|   MA|\n|01020|       CHICOPEE|[-72.576142, 42.1...|31495|   MA|\n|01022|   WESTOVER AFB|[-72.558657, 42.1...| 1764|   MA|\n|01026|     CUMMINGTON|[-72.905767, 42.4...| 1484|   MA|\n|01027|      MOUNT TOM|[-72.679921, 42.2...|16864|   MA|\n|01028|EAST LONGMEADOW|[-72.505565, 42.0...|13367|   MA|\n|01030|  FEEDING HILLS|[-72.675077, 42.0...|11985|   MA|\n|01031|   GILBERTVILLE|[-72.198585, 42.3...| 2385|   MA|\n|01032|         GOSHEN|[-72.844092, 42.4...|  122|   MA|\n|01033|         GRANBY|[-72.520001, 42.2...| 5526|   MA|\n|01034|        TOLLAND|[-72.908793, 42.0...| 1652|   MA|\n|01035|         HADLEY|[-72.571499, 42.3...| 4231|   MA|\n+-----+---------------+--------------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "spark.sql(\"select * from zipDSparquet\").show()", 
            "execution_count": 100, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Amazon S3", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### https://github.com/charles2588/bluemixsparknotebooks/blob/master/Python/read_write_S3.ipynb", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Amazon S3 - write parquet", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "zipDS.write.parquet(\"s3a://incoming5824/zipDS.parquet\")", 
            "execution_count": 101, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 102, 
                    "data": {
                        "text/plain": "Name: org.apache.spark.sql.AnalysisException\nMessage: path s3a://incoming5824/zipDS.parquet already exists.;\nStackTrace:   at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:88)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:525)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "// Even this command works, but u can run this or the above\nzipDS.write.save(\"s3a://incoming5824/zipDS.parquet\")", 
            "execution_count": 102, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "// Writes Parquet not CSV, ignore\n//zipDS.write.save(\"s3a://incoming5824/zipDS.csv\")", 
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## Amazon S3 - read parquet", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+--------------------+-----+-----+\n|  zip|           city|                 loc|  pop|state|\n+-----+---------------+--------------------+-----+-----+\n|01001|         AGAWAM|[-72.622739, 42.0...|15338|   MA|\n|01002|        CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n|01005|          BARRE|[-72.108354, 42.4...| 4546|   MA|\n|01007|    BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n|01008|      BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n|01010|      BRIMFIELD|[-72.188455, 42.1...| 3706|   MA|\n|01011|        CHESTER|[-72.988761, 42.2...| 1688|   MA|\n|01012|   CHESTERFIELD|[-72.833309, 42.3...|  177|   MA|\n|01013|       CHICOPEE|[-72.607962, 42.1...|23396|   MA|\n|01020|       CHICOPEE|[-72.576142, 42.1...|31495|   MA|\n|01022|   WESTOVER AFB|[-72.558657, 42.1...| 1764|   MA|\n|01026|     CUMMINGTON|[-72.905767, 42.4...| 1484|   MA|\n|01027|      MOUNT TOM|[-72.679921, 42.2...|16864|   MA|\n|01028|EAST LONGMEADOW|[-72.505565, 42.0...|13367|   MA|\n|01030|  FEEDING HILLS|[-72.675077, 42.0...|11985|   MA|\n|01031|   GILBERTVILLE|[-72.198585, 42.3...| 2385|   MA|\n|01032|         GOSHEN|[-72.844092, 42.4...|  122|   MA|\n|01033|         GRANBY|[-72.520001, 42.2...| 5526|   MA|\n|01034|        TOLLAND|[-72.908793, 42.0...| 1652|   MA|\n|01035|         HADLEY|[-72.571499, 42.3...| 4231|   MA|\n+-----+---------------+--------------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "val zipDSparquet = spark.read.parquet(\"s3a://incoming5824/zipDS.parquet\")\nzipDSparquet.show()", 
            "execution_count": 103, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+--------------------+-----+-----+\n|  zip|           city|                 loc|  pop|state|\n+-----+---------------+--------------------+-----+-----+\n|01001|         AGAWAM|[-72.622739, 42.0...|15338|   MA|\n|01002|        CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n|01005|          BARRE|[-72.108354, 42.4...| 4546|   MA|\n|01007|    BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n|01008|      BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n|01010|      BRIMFIELD|[-72.188455, 42.1...| 3706|   MA|\n|01011|        CHESTER|[-72.988761, 42.2...| 1688|   MA|\n|01012|   CHESTERFIELD|[-72.833309, 42.3...|  177|   MA|\n|01013|       CHICOPEE|[-72.607962, 42.1...|23396|   MA|\n|01020|       CHICOPEE|[-72.576142, 42.1...|31495|   MA|\n|01022|   WESTOVER AFB|[-72.558657, 42.1...| 1764|   MA|\n|01026|     CUMMINGTON|[-72.905767, 42.4...| 1484|   MA|\n|01027|      MOUNT TOM|[-72.679921, 42.2...|16864|   MA|\n|01028|EAST LONGMEADOW|[-72.505565, 42.0...|13367|   MA|\n|01030|  FEEDING HILLS|[-72.675077, 42.0...|11985|   MA|\n|01031|   GILBERTVILLE|[-72.198585, 42.3...| 2385|   MA|\n|01032|         GOSHEN|[-72.844092, 42.4...|  122|   MA|\n|01033|         GRANBY|[-72.520001, 42.2...| 5526|   MA|\n|01034|        TOLLAND|[-72.908793, 42.0...| 1652|   MA|\n|01035|         HADLEY|[-72.571499, 42.3...| 4231|   MA|\n+-----+---------------+--------------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "// // Even this command works, but u can run this or the above\nval zipDSparquet = (spark.read\n.format(\"org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\")\n.option(\"header\", \"true\")\n.load(\"s3a://incoming5824/zipDS.parquet\"))\nzipDSparquet.show()", 
            "execution_count": 109, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Amazon S3 - \"Query\" up-to-the-minute data from Parquet Table", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+-----------------------+-----+-----+\n|zip  |city           |loc                    |pop  |state|\n+-----+---------------+-----------------------+-----+-----+\n|01001|AGAWAM         |[-72.622739, 42.070206]|15338|MA   |\n|01002|CUSHMAN        |[-72.51565, 42.377017] |36963|MA   |\n|01005|BARRE          |[-72.108354, 42.409698]|4546 |MA   |\n|01007|BELCHERTOWN    |[-72.410953, 42.275103]|10579|MA   |\n|01008|BLANDFORD      |[-72.936114, 42.182949]|1240 |MA   |\n|01010|BRIMFIELD      |[-72.188455, 42.116543]|3706 |MA   |\n|01011|CHESTER        |[-72.988761, 42.279421]|1688 |MA   |\n|01012|CHESTERFIELD   |[-72.833309, 42.38167] |177  |MA   |\n|01013|CHICOPEE       |[-72.607962, 42.162046]|23396|MA   |\n|01020|CHICOPEE       |[-72.576142, 42.176443]|31495|MA   |\n|01022|WESTOVER AFB   |[-72.558657, 42.196672]|1764 |MA   |\n|01026|CUMMINGTON     |[-72.905767, 42.435296]|1484 |MA   |\n|01027|MOUNT TOM      |[-72.679921, 42.264319]|16864|MA   |\n|01028|EAST LONGMEADOW|[-72.505565, 42.067203]|13367|MA   |\n|01030|FEEDING HILLS  |[-72.675077, 42.07182] |11985|MA   |\n|01031|GILBERTVILLE   |[-72.198585, 42.332194]|2385 |MA   |\n|01032|GOSHEN         |[-72.844092, 42.466234]|122  |MA   |\n|01033|GRANBY         |[-72.520001, 42.255704]|5526 |MA   |\n|01034|TOLLAND        |[-72.908793, 42.070234]|1652 |MA   |\n|01035|HADLEY         |[-72.571499, 42.36062] |4231 |MA   |\n+-----+---------------+-----------------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "val parquetOutputPath1 = \"s3a://AKIAJ2SOSS5HJQBP3FMQ:Ga94v+bRU1nMX73aBenuGMSrR5T9wq1KhH4WHKvj@incoming5824/zipDS.parquet/\" \nval sqlDF = spark.sql(s\"SELECT * FROM parquet.`$parquetOutputPath1`\")\nsqlDF.show(false)", 
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "class org.apache.spark.sql.Dataset\n"
                }
            ], 
            "source": "println(sqlDF.getClass)", 
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "// This does not work , from Databricks scala notebook with streaming  data\nval parquetOutputPath1 = \"s3a://AKIAJ2SOSS5HJQBP3FMQ:Ga94v+bRU1nMX73aBenuGMSrR5T9wq1KhH4WHKvj@cloudtrail5824/\" \nval sqlDF = spark.sql(s\"SELECT * FROM parquet.`$parquetOutputPath1`\")\n//sqlDF.show(false)", 
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Reading from Bluemix Object Storage a csv file and writing the csv file to s3 and then reading it back in DSX", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Read from bluemix object store", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "1\n+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n| 0.29|Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n| 0.31|   Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n+-----+-------+-----+-------+-----+-----+-----+----+----+----+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "val dfData3 = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    option(\"inferSchema\", \"true\").\n    load(\"swift://Databricks.\" + name + \"/diamonds.csv\")\ndfData3.show(5)", 
            "execution_count": 111, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Write to s3 Object store", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "dfData3.write.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").save(\"s3a://incoming5824/diamonds_out.csv\")", 
            "execution_count": 112, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Read from s3 object store", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----+---------+---+----+----+----+---+----+----+----+\n|0.23|    Ideal|  E| SI2|61.5|55.0|326|3.95|3.98|2.43|\n+----+---------+---+----+----+----+---+----+----+----+\n|0.21|  Premium|  E| SI1|59.8|61.0|326|3.89|3.84|2.31|\n|0.23|     Good|  E| VS1|56.9|65.0|327|4.05|4.07|2.31|\n|0.29|  Premium|  I| VS2|62.4|58.0|334| 4.2|4.23|2.63|\n|0.31|     Good|  J| SI2|63.3|58.0|335|4.34|4.35|2.75|\n|0.24|Very Good|  J|VVS2|62.8|57.0|336|3.94|3.96|2.48|\n+----+---------+---+----+----+----+---+----+----+----+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "val dfData2 = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    option(\"inferSchema\", \"true\").\n    load(\"swift://Databricks.\" + name + \"/diamonds_out.csv\")\ndfData2.show(5)", 
            "execution_count": 113, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Reading from Amazon S3 Bucket, a file that was written by DataConnect", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n| 0.29|Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n| 0.31|   Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n+-----+-------+-----+-------+-----+-----+-----+----+----+----+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "val dfData1 = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    option(\"inferSchema\", \"true\").\n    load(\"s3a://incoming5824/diamonds_dataconnect\")\ndfData1.show(5)", 
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "val primitiveDS = Seq(1, 2, 3).toDS()", 
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 19, 
                    "data": {
                        "text/plain": "class org.apache.spark.sql.Dataset"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "primitiveDS.getClass", 
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 20, 
                    "data": {
                        "text/plain": "Array(2, 3, 4)"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)", 
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat_minor": 0
}