{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "name": "scala", 
            "version": "2.11.8"
        }, 
        "kernelspec": {
            "language": "scala", 
            "name": "scala-spark20", 
            "display_name": "Scala 2.11 with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "source": "# Overview (SCALA)\n## How to Use SparkSession - A Unified Entry Point in Apache Spark 2.0\n\n\nIn Spark 2.0, SparkSession, is a new entry point that subsumes SparkContext, SQLContext, StreamingContext, and HiveContext. For backward compatibiilty, they are preserved. SparkSession has many features, and in this notebook , by way of simple code examples, some of the more important ones, using data to illustrate its access to underlying Spark functionality. Even though, this notebook is written in Scala, similar functionality and APIs exist in Python and Java.\nIn DSX notebooks and Spark REPL, the SparkSession is created for you, stored in a variable called spark.\n\nThe companion blog post http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSession.html and https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# PART 1: Exploring SparkSession\nFor backward compatibility, you can access SparkContext, SQLContext, and SparkConf", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "// http://www.agildata.com/apache-spark-rdd-vs-dataframe-vs-dataset/\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 79, 
            "source": "spark", 
            "outputs": [
                {
                    "execution_count": 79, 
                    "data": {
                        "text/plain": "org.apache.spark.sql.SparkSession@f44036ea"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## SparkContext as part of SparkSession\nPreserved as part of SparkSession for backward compatibility.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 2, 
            "source": "spark.sparkContext", 
            "outputs": [
                {
                    "execution_count": 2, 
                    "data": {
                        "text/plain": "org.apache.spark.SparkContext@de6f89d8"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## sqlContext as part of SparkSession\nPreserved as part of SparkSession for backward compatibility", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 3, 
            "source": "spark.sqlContext", 
            "outputs": [
                {
                    "execution_count": 3, 
                    "data": {
                        "text/plain": "org.apache.spark.sql.SQLContext@fb6615b7"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Configuring Spark's runtime configuration parameters", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## SparkConf as part of SparkSession\nThrough spark.conf, You manipulate Spark's runtime configruation parameters. Note that all configuration options set are automatically propagated over to Spark and Hadoop during I/O.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 4, 
            "source": "spark.conf.set(\"spark.notebook.name\", \"SparkSessionSimpleZipExample\")", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 5, 
            "source": "spark.conf.get(\"spark.notebook.name\")", 
            "outputs": [
                {
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "SparkSessionSimpleZipExample"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 6, 
            "source": "spark.conf.get(\"spark.sql.warehouse.dir\")", 
            "outputs": [
                {
                    "execution_count": 6, 
                    "data": {
                        "text/plain": "file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Spark config variables set can be accessed via SQL with variable subsitution", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 7, 
            "source": "spark.sql(\"select '${spark.notebook.name}', '${spark.sql.warehouse.dir}'\")", 
            "outputs": [
                {
                    "execution_count": 7, 
                    "data": {
                        "text/plain": "[SparkSessionSimpleZipExample: string, ${spark.sql.warehouse.dir}: string]"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Creating DataFrames and Datasets\nThere are a number of ways to create DataFrames and Datasets using the SparkSession APIs. Once either a DataFrame or Dataset is created, you can manipulate your data. For example, for quick exploration of Datasets, you can use the spark.range", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 8, 
            "source": "import org.apache.spark.sql.functions._", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 9, 
            "source": "val numDS = spark.range(5, 100, 5)\nnumDS.show(5)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+\n| id|\n+---+\n|  5|\n| 10|\n| 15|\n| 20|\n| 25|\n+---+\nonly showing top 5 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 10, 
            "source": "numDS.describe().show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-------+------------------+\n|summary|                id|\n+-------+------------------+\n|  count|                19|\n|   mean|              50.0|\n| stddev|28.136571693556885|\n|    min|                 5|\n|    max|                95|\n+-------+------------------+\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Creating a DataFrame from a collection with SparkSession", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 11, 
            "source": "val langPercentDF = spark.createDataFrame(List((\"Scala\", 35), (\"Python\", 30), (\"R\", 15), (\"Java\", 20)))", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 12, 
            "source": "val lpDF = langPercentDF.withColumnRenamed(\"_1\", \"language\").withColumnRenamed(\"_2\", \"percent\")", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 13, 
            "source": "lpDF.orderBy(desc(\"percent\")).show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+-------+\n|language|percent|\n+--------+-------+\n|   Scala|     35|\n|  Python|     30|\n|    Java|     20|\n|       R|     15|\n+--------+-------+\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# PART 2: Exploring Zip codes data using SparkSession and Dataset APIs.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Next, we going to exlore some zip code data fetched from MongoDB", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 14, 
            "source": "import sys.process._\n\"wget http://media.mongodb.org/zips.json\" !", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "--2017-01-27 14:51:59--  http://media.mongodb.org/zips.json\nResolving media.mongodb.org (media.mongodb.org)... 52.85.202.138, 52.85.202.249, 52.85.202.70, ...\nConnecting to media.mongodb.org (media.mongodb.org)|52.85.202.138|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3182409 (3.0M) [application/json]\nSaving to: \u2018zips.json.3\u2019\n\n     0K .......... .......... .......... .......... ..........  1%  150M 0s\n    50K .......... .......... .......... .......... ..........  3% 36.0M 0s\n   100K .......... .......... .......... .......... ..........  4% 37.1M 0s\n   150K .......... .......... .......... .......... ..........  6% 13.9M 0s\n   200K .......... .......... .......... .......... ..........  8% 99.4M 0s\n   250K .......... .......... .......... .......... ..........  9% 18.5M 0s\n   300K .......... .......... .......... .......... .......... 11% 19.2M 0s\n   350K .......... .......... .......... .......... .......... 12% 42.8M 0s\n   400K .......... .......... .......... .......... .......... 14% 11.7M 0s\n   450K .......... .......... .......... .......... .......... 16%  122M 0s\n   500K .......... .......... .......... .......... .......... 17% 7.59M 0s\n   550K .......... .......... .......... .......... .......... 19%  113M 0s\n   600K .......... .......... .......... .......... .......... 20% 38.8M 0s\n   650K .......... .......... .......... .......... .......... 22% 97.4M 0s\n   700K .......... .......... .......... .......... .......... 24% 25.3M 0s\n   750K .......... .......... .......... .......... .......... 25% 25.8M 0s\n   800K .......... .......... .......... .......... .......... 27% 29.1M 0s\n   850K .......... .......... .......... .......... .......... 28% 19.9M 0s\n   900K .......... .......... .......... .......... .......... 30%  123M 0s\n   950K .......... .......... .......... .......... .......... 32% 9.06M 0s\n  1000K .......... .......... .......... .......... .......... 33% 88.9M 0s\n  1050K .......... .......... .......... .......... .......... 35% 12.4M 0s\n  1100K .......... .......... .......... .......... .......... 37% 98.4M 0s\n  1150K .......... .......... .......... .......... .......... 38%  102M 0s\n  1200K .......... .......... .......... .......... .......... 40% 14.3M 0s\n  1250K .......... .......... .......... .......... .......... 41% 28.6M 0s\n  1300K .......... .......... .......... .......... .......... 43% 23.6M 0s\n  1350K .......... .......... .......... .......... .......... 45%  109M 0s\n  1400K .......... .......... .......... .......... .......... 46% 18.9M 0s\n  1450K .......... .......... .......... .......... .......... 48% 3.23M 0s\n  1500K .......... .......... .......... .......... .......... 49% 92.4M 0s\n  1550K .......... .......... .......... .......... .......... 51% 8.69M 0s\n  1600K .......... .......... .......... .......... .......... 53% 97.1M 0s\n  1650K .......... .......... .......... .......... .......... 54%  112M 0s\n  1700K .......... .......... .......... .......... .......... 56% 13.3M 0s\n  1750K .......... .......... .......... .......... .......... 57% 22.4M 0s\n  1800K .......... .......... .......... .......... .......... 59%  272M 0s\n  1850K .......... .......... .......... .......... .......... 61% 11.9M 0s\n  1900K .......... .......... .......... .......... .......... 62% 96.2M 0s\n  1950K .......... .......... .......... .......... .......... 64% 6.61M 0s\n  2000K .......... .......... .......... .......... .......... 65% 18.2M 0s\n  2050K .......... .......... .......... .......... .......... 67%  118M 0s\n  2100K .......... .......... .......... .......... .......... 69% 5.33M 0s\n  2150K .......... .......... .......... .......... .......... 70%  113M 0s\n  2200K .......... .......... .......... .......... .......... 72%  109M 0s\n  2250K .......... .......... .......... .......... .......... 74%  104M 0s\n  2300K .......... .......... .......... .......... .......... 75% 95.3M 0s\n  2350K .......... .......... .......... .......... .......... 77% 10.1M 0s\n  2400K .......... .......... .......... .......... .......... 78% 99.7M 0s\n  2450K .......... .......... .......... .......... .......... 80% 64.8M 0s\n  2500K .......... .......... .......... .......... .......... 82%  118M 0s\n  2550K .......... .......... .......... .......... .......... 83%  905K 0s\n  2600K .......... .......... .......... .......... .......... 85% 5.17M 0s\n  2650K .......... .......... .......... .......... .......... 86%  107M 0s\n  2700K .......... .......... .......... .......... .......... 88% 92.8M 0s\n  2750K .......... .......... .......... .......... .......... 90% 5.84M 0s\n  2800K .......... .......... .......... .......... .......... 91% 2.31M 0s\n  2850K .......... .......... .......... .......... .......... 93%  393M 0s\n  2900K .......... .......... .......... .......... .......... 94%  110M 0s\n  2950K .......... .......... .......... .......... .......... 96%  123M 0s\n  3000K .......... .......... .......... .......... .......... 98% 88.2M 0s\n  3050K .......... .......... .......... .......... .......... 99% 99.5M 0s\n  3100K .......                                               100%  259M=0.2s\n\n2017-01-27 14:52:00 (14.2 MB/s) - \u2018zips.json.3\u2019 saved [3182409/3182409]\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "##### The above command runs on your cluster's single node, fetches the zip code file from the specified URL, unzips in the directory below", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 15, 
            "source": "\"pwd\" !", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 16, 
            "source": "\"ls\" !", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "cloudant_credentials.json\nexample.txt\nml-1m\nml-1m.zip\nmtcars.csv\nmyPandasData.csv\nratings.dat\nrecommender_model\nspark-warehouse\nsubmission.csv\nzips.json\nzips.json.1\nzips.json.2\nzips.json.3\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Reading the JSON file with SparkSession\n### Read the JSON file, infer the schema and convert it into a Dataset dictated by the case class Zips", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 2, 
            "source": "import org.apache.spark.sql.SparkSession\nval spark = (SparkSession.\n    builder().\n    getOrCreate())\n// For implicit conversions like converting RDDs to DataFrames\nimport spark.implicits._", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 3, 
            "source": "import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\nimport org.apache.spark.sql.Encoder\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n// A case class for zips data\ncase class Zips(zip:String, city:String, loc:Array[Double], pop:Long, state:String)", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 4, 
            "source": "val zipDF = spark.read.json(\"zips.json\").withColumnRenamed(\"_id\",\"zip\")\n//rename the _id to zip for readability\n//convert to a dataset using the case class\n//val zipDS = zipDF.withColumnRenamed(\"_id\",\"zip\").as[Zips]\nval zipDS = zipDF.as[Zips]\n// since we will be quering this dataset often let's cache it\nzipDS.cache()\n//display(zipDS)\nzipDS.show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+--------------------+-----+-----+\n|  zip|           city|                 loc|  pop|state|\n+-----+---------------+--------------------+-----+-----+\n|01001|         AGAWAM|[-72.622739, 42.0...|15338|   MA|\n|01002|        CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n|01005|          BARRE|[-72.108354, 42.4...| 4546|   MA|\n|01007|    BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n|01008|      BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n|01010|      BRIMFIELD|[-72.188455, 42.1...| 3706|   MA|\n|01011|        CHESTER|[-72.988761, 42.2...| 1688|   MA|\n|01012|   CHESTERFIELD|[-72.833309, 42.3...|  177|   MA|\n|01013|       CHICOPEE|[-72.607962, 42.1...|23396|   MA|\n|01020|       CHICOPEE|[-72.576142, 42.1...|31495|   MA|\n|01022|   WESTOVER AFB|[-72.558657, 42.1...| 1764|   MA|\n|01026|     CUMMINGTON|[-72.905767, 42.4...| 1484|   MA|\n|01027|      MOUNT TOM|[-72.679921, 42.2...|16864|   MA|\n|01028|EAST LONGMEADOW|[-72.505565, 42.0...|13367|   MA|\n|01030|  FEEDING HILLS|[-72.675077, 42.0...|11985|   MA|\n|01031|   GILBERTVILLE|[-72.198585, 42.3...| 2385|   MA|\n|01032|         GOSHEN|[-72.844092, 42.4...|  122|   MA|\n|01033|         GRANBY|[-72.520001, 42.2...| 5526|   MA|\n|01034|        TOLLAND|[-72.908793, 42.0...| 1652|   MA|\n|01035|         HADLEY|[-72.571499, 42.3...| 4231|   MA|\n+-----+---------------+--------------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Q1: Can you display states, zips, cities with population greater than 40000, in descending order", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 5, 
            "source": "//display(zipDS.select(\"state\", \"city\", \"zip\", \"pop\").filter(\"pop > 40000\").orderBy(desc(\"pop\")))\nzipDS.select(\"state\", \"city\", \"zip\", \"pop\").filter(\"pop > 40000\").orderBy(desc(\"pop\")).show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+-----+------+\n|state|           city|  zip|   pop|\n+-----+---------------+-----+------+\n|   IL|        CHICAGO|60623|112047|\n|   NY|       BROOKLYN|11226|111396|\n|   NY|       NEW YORK|10021|106564|\n|   NY|       NEW YORK|10025|100027|\n|   CA|   BELL GARDENS|90201| 99568|\n|   IL|        CHICAGO|60617| 98612|\n|   CA|    LOS ANGELES|90011| 96074|\n|   IL|        CHICAGO|60647| 95971|\n|   IL|        CHICAGO|60628| 94317|\n|   CA|        NORWALK|90650| 94188|\n|   IL|        CHICAGO|60620| 92005|\n|   IL|        CHICAGO|60629| 91814|\n|   IL|        CHICAGO|60609| 89762|\n|   IL|        CHICAGO|60618| 88377|\n|   NY|JACKSON HEIGHTS|11373| 88241|\n|   CA|         ARLETA|91331| 88114|\n|   NY|       BROOKLYN|11212| 87079|\n|   CA|     SOUTH GATE|90280| 87026|\n|   NY|      RIDGEWOOD|11385| 85732|\n|   NY|          BRONX|10467| 85710|\n+-----+---------------+-----+------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Q2: Which cities and zips in the state of california are most populous?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 6, 
            "source": "//display(zipDS.select(\"city\", \"zip\", \"pop\").filter('state === \"CA\").orderBy(desc(\"pop\")))\nzipDS.select(\"city\", \"zip\", \"pop\").filter('state === \"CA\").orderBy(desc(\"pop\")).show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+-----+-----+\n|            city|  zip|  pop|\n+----------------+-----+-----+\n|    BELL GARDENS|90201|99568|\n|     LOS ANGELES|90011|96074|\n|         NORWALK|90650|94188|\n|          ARLETA|91331|88114|\n|      SOUTH GATE|90280|87026|\n|     LOS ANGELES|90044|83958|\n|         FONTANA|92335|81255|\n|      HOLLY PARK|90250|78511|\n|     WESTMINSTER|92683|77965|\n|       SANTA ANA|92704|77151|\n|        INDUSTRY|91744|77114|\n|COAST GUARD ISLA|94501|76110|\n|          RIALTO|92376|75341|\n|     LOS ANGELES|90026|74751|\n|      LONG BEACH|90805|74011|\n| HUNTINGTON PARK|90255|72139|\n|   MORENO VALLEY|92553|71314|\n|LAKE LOS ANGELES|93550|71024|\n|   SAN FRANCISCO|94110|70770|\n|       IRWINDALE|91706|69464|\n+----------------+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Q3: Can you sum up the population of all the states and order them in descending order?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 8, 
            "source": "//display(zipDS.select(\"state\", \"pop\").groupBy(\"state\").sum(\"pop\").orderBy(desc(\"sum(pop)\")))\nzipDS.select(\"state\", \"pop\").groupBy(\"state\").sum(\"pop\").orderBy(desc(\"sum(pop)\")).show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "\r                                                                                \r+-----+--------+\n|state|sum(pop)|\n+-----+--------+\n|   CA|29754890|\n|   NY|17990402|\n|   TX|16984601|\n|   FL|12686644|\n|   PA|11881643|\n|   IL|11427576|\n|   OH|10846517|\n|   MI| 9295297|\n|   NJ| 7730188|\n|   NC| 6628637|\n|   GA| 6478216|\n|   VA| 6181479|\n|   MA| 6016425|\n|   IN| 5544136|\n|   MO| 5110648|\n|   WI| 4891769|\n|   TN| 4876457|\n|   WA| 4866692|\n|   MD| 4781379|\n|   MN| 4372982|\n+-----+--------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# PART 3: Creating Hive Table, registering a UDF, and querying it using SparkSession and Spark SQL APIs", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### drop the table if one exists", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 19, 
            "source": "spark.sql(\"DROP TABLE IF EXISTS hive_zips_table\")", 
            "outputs": [
                {
                    "execution_count": 19, 
                    "data": {
                        "text/plain": "[]"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 20, 
            "source": "import sys.process._", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### Just ensure we don't have any lingering files in the directory because of eventual consistency.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 21, 
            "source": "\"ls /gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/hive_zips_table\" !", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "part-r-00000-9d0f09b9-4830-4a08-a874-2749cf118332.snappy.parquet\n_SUCCESS\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 22, 
            "source": " \"rm -rf /gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/hive_zips_table\" !", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 23, 
            "source": "zipDS.write.saveAsTable(\"hive_zips_table\")", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Working and Accessing Catalog metadata", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 24, 
            "source": "//display(spark.catalog.listDatabases)\nspark.catalog.listDatabases.show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-------+----------------+--------------------+\n|   name|     description|         locationUri|\n+-------+----------------+--------------------+\n|default|default database|file:/gpfs/global...|\n+-------+----------------+--------------------+\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 25, 
            "source": "//display(spark.catalog.listTables)\nspark.catalog.listTables.show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------------+--------+-----------+---------+-----------+\n|           name|database|description|tableType|isTemporary|\n+---------------+--------+-----------+---------+-----------+\n|hive_zips_table| default|       null|  MANAGED|      false|\n+---------------+--------+-----------+---------+-----------+\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Cache table using SparkSession API", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 26, 
            "source": "spark.catalog.cacheTable(\"hive_zips_table\")", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## Q1: Can you query the Hive table with the Spark SQL query indentical to the one above Q1?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 27, 
            "source": "//display(spark.sql(\"SELECT state, city, zip, pop FROM hive_zips_table WHERE pop > 40000 ORDER BY pop DESC\"))\nspark.sql(\"SELECT state, city, zip, pop FROM hive_zips_table WHERE pop > 40000 ORDER BY pop DESC\").show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+---------------+-----+------+\n|state|           city|  zip|   pop|\n+-----+---------------+-----+------+\n|   IL|        CHICAGO|60623|112047|\n|   NY|       BROOKLYN|11226|111396|\n|   NY|       NEW YORK|10021|106564|\n|   NY|       NEW YORK|10025|100027|\n|   CA|   BELL GARDENS|90201| 99568|\n|   IL|        CHICAGO|60617| 98612|\n|   CA|    LOS ANGELES|90011| 96074|\n|   IL|        CHICAGO|60647| 95971|\n|   IL|        CHICAGO|60628| 94317|\n|   CA|        NORWALK|90650| 94188|\n|   IL|        CHICAGO|60620| 92005|\n|   IL|        CHICAGO|60629| 91814|\n|   IL|        CHICAGO|60609| 89762|\n|   IL|        CHICAGO|60618| 88377|\n|   NY|JACKSON HEIGHTS|11373| 88241|\n|   CA|         ARLETA|91331| 88114|\n|   NY|       BROOKLYN|11212| 87079|\n|   CA|     SOUTH GATE|90280| 87026|\n|   NY|      RIDGEWOOD|11385| 85732|\n|   NY|          BRONX|10467| 85710|\n+-----+---------------+-----+------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Q2: Find the populus cities in Calfornia with total number of zips using the hive table?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 28, 
            "source": "// display(spark.sql(\"SELECT COUNT(zip), SUM(pop), city FROM hive_zips_table WHERE state = 'CA' GROUP BY city ORDER BY SUM(pop) DESC\"))\nspark.sql(\"SELECT COUNT(zip), SUM(pop), city FROM hive_zips_table WHERE state = 'CA' GROUP BY city ORDER BY SUM(pop) DESC\").show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------+--------+----------------+\n|count(zip)|sum(pop)|            city|\n+----------+--------+----------------+\n|        56| 2102295|     LOS ANGELES|\n|        34| 1049298|       SAN DIEGO|\n|        29|  816653|        SAN JOSE|\n|        26|  723993|   SAN FRANCISCO|\n|        28|  628279|      SACRAMENTO|\n|        12|  347905|          FRESNO|\n|        12|  314487|         OAKLAND|\n|         8|  299651|      LONG BEACH|\n|         7|  272327|         ANAHEIM|\n|         8|  271347|     BAKERSFIELD|\n|        11|  267258|        STOCKTON|\n|         7|  253478|       RIVERSIDE|\n|         4|  234472|       SANTA ANA|\n|         5|  216459|         MODESTO|\n|         4|  183542|HUNTINGTON BEACH|\n|         7|  177552|  SAN BERNARDINO|\n|         4|  173374|         FREMONT|\n|         8|  163666|        GLENDALE|\n|         6|  158398|      SANTA ROSA|\n|         6|  158183|        TORRANCE|\n+----------+--------+----------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Registring a UDF with SparkSession", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Q4: Can you register a simple UDF with SparkSession that converts zip into long (currently it's a string)?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 29, 
            "source": "spark.sql(\"describe hive_zips_table\").show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------+-------------+-------+\n|col_name|    data_type|comment|\n+--------+-------------+-------+\n|     zip|       string|   null|\n|    city|       string|   null|\n|     loc|array<double>|   null|\n|     pop|       bigint|   null|\n|   state|       string|   null|\n+--------+-------------+-------+\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 30, 
            "source": "spark.udf.register(\"zipToLong\", (z:String) => z.toLong)", 
            "outputs": [
                {
                    "execution_count": 30, 
                    "data": {
                        "text/plain": "UserDefinedFunction(<function1>,LongType,Some(List(StringType)))"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 31, 
            "source": "spark.sql(\"SELECT city, zipToLong(zip) as zip_to_long FROM hive_zips_table ORDER BY zip_to_long DESC\").show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----------+-----------+\n|       city|zip_to_long|\n+-----------+-----------+\n|  KETCHIKAN|      99950|\n|   WRANGELL|      99929|\n|POINT BAKER|      99927|\n| METLAKATLA|      99926|\n|    KLAWOCK|      99925|\n|      HYDER|      99923|\n|   HYDABURG|      99922|\n|      CRAIG|      99921|\n| THORNE BAY|      99919|\n|  KETCHIKAN|      99901|\n|    SKAGWAY|      99840|\n|      SITKA|      99835|\n| PETERSBURG|      99833|\n|     HOONAH|      99829|\n|     HAINES|      99827|\n|   GUSTAVUS|      99826|\n|    DOUGLAS|      99824|\n|     ANGOON|      99820|\n|     JUNEAU|      99801|\n|    NUIQSUT|      99789|\n+-----------+-----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Register another UDF that calculates the strlen of cities", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 32, 
            "source": "spark.udf.register(\"cityLength\", (c:String) => c.length())", 
            "outputs": [
                {
                    "execution_count": 32, 
                    "data": {
                        "text/plain": "UserDefinedFunction(<function1>,IntegerType,Some(List(StringType)))"
                    }, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Using catalog data, get the list of your registered UDFs", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 33, 
            "source": "val udfs = spark.catalog.listFunctions()", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 34, 
            "source": "udfs.filter('name === \"cityLength\".toLowerCase || 'name === \"zipToLong\".toLowerCase).select(\"name\", \"database\").show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------+--------+\n|      name|database|\n+----------+--------+\n|citylength|    null|\n| ziptolong|    null|\n+----------+--------+\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 103, 
            "source": "spark.sql(\"SELECT city, cityLength(city) as city_length FROM hive_zips_table ORDER BY city_length DESC\").show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+-----------+\n|            city|city_length|\n+----------------+-----------+\n|CHEBEAGUE ISLAND|         16|\n|MONTGOMERY CENTE|         16|\n|CUMBERLAND CENTE|         16|\n|WEST BRIDGEWATER|         16|\n|OLD ORCHARD BEAC|         16|\n|WEST SPRINGFIELD|         16|\n|CUMBERLAND FORES|         16|\n|GREAT BARRINGTON|         16|\n|NORTH WHITEFIELD|         16|\n|NORTH CHELMSFORD|         16|\n|EAST MILLINOCKET|         16|\n|NEWTON UPPER FAL|         16|\n|GREENVILLE JUNCT|         16|\n|GILMANTON IRON W|         16|\n|LITTLE DEER ISLE|         16|\n|WOOD RIVER JUNCT|         16|\n|SOUTH GOULDSBORO|         16|\n|CENTER BARNSTEAD|         16|\n|SOUTHWEST HARBOR|         16|\n|WEST CHESTERFIEL|         16|\n+----------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Q5: Can you compose the same query as Q2 using Datasets APIs?", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 106, 
            "source": "(zipDS.filter('state === \"CA\")\n  .select(\"zip\", \"pop\", \"city\")\n  .groupBy(\"city\")\n  .sum()\n  .orderBy(desc(\"sum(pop)\")).show())", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "\r                                                                                \r+----------------+--------+\n|            city|sum(pop)|\n+----------------+--------+\n|     LOS ANGELES| 2102295|\n|       SAN DIEGO| 1049298|\n|        SAN JOSE|  816653|\n|   SAN FRANCISCO|  723993|\n|      SACRAMENTO|  628279|\n|          FRESNO|  347905|\n|         OAKLAND|  314487|\n|      LONG BEACH|  299651|\n|         ANAHEIM|  272327|\n|     BAKERSFIELD|  271347|\n|        STOCKTON|  267258|\n|       RIVERSIDE|  253478|\n|       SANTA ANA|  234472|\n|         MODESTO|  216459|\n|HUNTINGTON BEACH|  183542|\n|  SAN BERNARDINO|  177552|\n|         FREMONT|  173374|\n|        GLENDALE|  163666|\n|      SANTA ROSA|  158398|\n|        TORRANCE|  158183|\n+----------------+--------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 69, 
            "source": "(zipDS.filter('state === \"CA\")\n  .select(\"zip\", \"pop\", \"city\")\n  .groupBy(\"city\")\n  .agg(sum(\"pop\").alias(\"population\"))\n  .orderBy(desc(\"population\"))).show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "\r                                                                                \r+----------------+----------+\n|            city|population|\n+----------------+----------+\n|     LOS ANGELES|   2102295|\n|       SAN DIEGO|   1049298|\n|        SAN JOSE|    816653|\n|   SAN FRANCISCO|    723993|\n|      SACRAMENTO|    628279|\n|          FRESNO|    347905|\n|         OAKLAND|    314487|\n|      LONG BEACH|    299651|\n|         ANAHEIM|    272327|\n|     BAKERSFIELD|    271347|\n|        STOCKTON|    267258|\n|       RIVERSIDE|    253478|\n|       SANTA ANA|    234472|\n|         MODESTO|    216459|\n|HUNTINGTON BEACH|    183542|\n|  SAN BERNARDINO|    177552|\n|         FREMONT|    173374|\n|        GLENDALE|    163666|\n|      SANTA ROSA|    158398|\n|        TORRANCE|    158183|\n+----------------+----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 71, 
            "source": "(zipDS.filter('state === \"CA\")\n  .select(\"zip\", \"pop\", \"city\")\n  .groupBy(\"city\")\n  .agg(sum(\"pop\").alias(\"population\"), count(\"*\").alias(\"cnt\"))\n  .orderBy(desc(\"population\"))).show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------------+----------+---+\n|            city|population|cnt|\n+----------------+----------+---+\n|     LOS ANGELES|   2102295| 56|\n|       SAN DIEGO|   1049298| 34|\n|        SAN JOSE|    816653| 29|\n|   SAN FRANCISCO|    723993| 26|\n|      SACRAMENTO|    628279| 28|\n|          FRESNO|    347905| 12|\n|         OAKLAND|    314487| 12|\n|      LONG BEACH|    299651|  8|\n|         ANAHEIM|    272327|  7|\n|     BAKERSFIELD|    271347|  8|\n|        STOCKTON|    267258| 11|\n|       RIVERSIDE|    253478|  7|\n|       SANTA ANA|    234472|  4|\n|         MODESTO|    216459|  5|\n|HUNTINGTON BEACH|    183542|  4|\n|  SAN BERNARDINO|    177552|  7|\n|         FREMONT|    173374|  4|\n|        GLENDALE|    163666|  8|\n|      SANTA ROSA|    158398|  6|\n|        TORRANCE|    158183|  6|\n+----------------+----------+---+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": 74, 
            "source": "(zipDS.filter('state === \"CA\")\n  .select(\"zip\", \"pop\", \"city\")\n  .groupBy(\"city\")\n  .agg(sum(\"pop\").alias(\"population\"), count(\"zip\").alias(\"zip\"))\n  .orderBy(desc(\"population\"))).show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "\r                                                                                \r+----------------+----------+---+\n|            city|population|zip|\n+----------------+----------+---+\n|     LOS ANGELES|   2102295| 56|\n|       SAN DIEGO|   1049298| 34|\n|        SAN JOSE|    816653| 29|\n|   SAN FRANCISCO|    723993| 26|\n|      SACRAMENTO|    628279| 28|\n|          FRESNO|    347905| 12|\n|         OAKLAND|    314487| 12|\n|      LONG BEACH|    299651|  8|\n|         ANAHEIM|    272327|  7|\n|     BAKERSFIELD|    271347|  8|\n|        STOCKTON|    267258| 11|\n|       RIVERSIDE|    253478|  7|\n|       SANTA ANA|    234472|  4|\n|         MODESTO|    216459|  5|\n|HUNTINGTON BEACH|    183542|  4|\n|  SAN BERNARDINO|    177552|  7|\n|         FREMONT|    173374|  4|\n|        GLENDALE|    163666|  8|\n|      SANTA ROSA|    158398|  6|\n|        TORRANCE|    158183|  6|\n+----------------+----------+---+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "execution_count": null, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4
}