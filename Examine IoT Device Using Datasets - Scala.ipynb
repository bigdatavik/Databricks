{
    "nbformat_minor": 0, 
    "nbformat": 4, 
    "cells": [
        {
            "source": "# How to Process IoT Device JSON Data Using Dataset", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "Datasets in Apache Spark 2.0 provide high-level domain specific APIs as well as provide structure and compile-time type-safety. You can read your JSON data file into a DataFrame, a generic row of JVM objects, and convert them into type-specific collection of JVM objects.\nIn this notebook, you read a JSON file, convert the semi-structured JSON data into a collection of Datasets[T], and work with some high-level Spark 2.0 Dataset APIs.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Spark supports multiple formats : JSON, CSV, Text, Parquet, ORC etc. To read a JSON file, you can simply use the SparkSession handle spark.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In previous versions of Spark, you had to create a SparkConf and SparkContext to interact with Spark (http://bit.ly/2jOyWCE)\n\n- //set up the spark configuration and create contexts\n- val sparkConf = new SparkConf().setAppName(\"SparkSessionZipsExample\").setMaster(\"local\")\n- // your handle to SparkContext to access other context like SQLContext\n- val sc = new SparkContext(sparkConf).set(\"spark.some.config.option\", \"some-value\")\n- val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\nWhereas in Spark 2.0 the same effects can be achieved through SparkSession, without expliciting creating SparkConf, SparkContext or SQLContext, as they\u2019re encapsulated within the SparkSession. Using a builder design pattern, it instantiates a SparkSession object if one does not already exist, along with its associated underlying contexts.\n- // Create a SparkSession. No need to create SparkContext\n- // You automatically get it as part of the SparkSession\n- val warehouseLocation = \"file:${system:user.dir}/spark-warehouse\"\n- val spark = SparkSession\n- .builder()\n- .appName(\"SparkSessionZipsExample\")\n- .config(\"spark.sql.warehouse.dir\", warehouseLocation)\n- .enableHiveSupport()\n- .getOrCreate()\n\nAt this point you can use the spark variable as your instance object to access its public methods and instances for the duration of your Spark job.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "spark", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "org.apache.spark.sql.SparkSession@9a1777e6"
                    }, 
                    "execution_count": 1, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "spark.sparkContext", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "org.apache.spark.SparkContext@2efc56e8"
                    }, 
                    "execution_count": 2, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "spark.sqlContext", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "org.apache.spark.sql.SQLContext@23ddb267"
                    }, 
                    "execution_count": 3, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "spark.conf", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "org.apache.spark.sql.RuntimeConfig@fa33f052"
                    }, 
                    "execution_count": 4, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "sc", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "org.apache.spark.SparkContext@7cb5ce9f"
                    }, 
                    "execution_count": 5, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "spark.conf.get(\"spark.sql.warehouse.dir\")", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/"
                    }, 
                    "execution_count": 6, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Reading JSON as a Dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Create a case class to represent your IoT Device Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// define a case class that represents our Device data.\n// Use the Scala case class DeviceIoTData to convert the JSON device data into a Scala object.\n\n// Import implicts or error while loading the dataset below - Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._ \nimport spark.implicits._ \n\ncase class DeviceIoTData (\n device_id: Long,  \n device_name: String,\n ip: String,\n cca2: String,\n cca3: String,\n cn: String,\n latitude: Double,\n longitude: Double,\n scale: String,\n temp: Long,\n humidity: Long,\n battery_level: Long,\n c02_level: Long,\n lcd: String,\n timestamp: Long\n)", 
            "outputs": [], 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "Spark supports multiple formats : JSON, CSV, Text, Parquet, ORC etc. To read a JSON file, you can simply use the SparkSession handle spark.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import org.apache.spark.sql.SparkSession\n\n// @hidden_cell\n// This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n// You might want to remove those credentials before you share your notebook.\ndef setHadoopConfig19099026f8df40b6aec4353c7e897e95(name: String) = {\n    // This function sets the Hadoop configuration so it is possible to\n    // access data from Bluemix Object Storage using Spark\n\n    val prefix = \"fs.swift.service.\" + name\n    sc.hadoopConfiguration.set(prefix + \".auth.url\", \"https://identity.open.softlayer.com\" + \"/v3/auth/tokens\")\n    sc.hadoopConfiguration.set(prefix + \".auth.endpoint.prefix\",\"endpoints\")\n    sc.hadoopConfiguration.set(prefix + \".tenant\", \"cc29768790ec45439a43668592b02f84\")\n    sc.hadoopConfiguration.set(prefix + \".username\", \"a55ccc8b825944fa90f0188f8e5a2ffc\")\n    sc.hadoopConfiguration.set(prefix + \".password\", \"Q#i79zYI{qV?d74u\")\n    sc.hadoopConfiguration.setInt(prefix + \".http.port\", 8080)\n    sc.hadoopConfiguration.set(prefix + \".region\", \"dallas\")\n    sc.hadoopConfiguration.setBoolean(prefix + \".public\", false)\n}\n\n// you can choose any name\nval name = \"keystone\"\nsetHadoopConfig19099026f8df40b6aec4353c7e897e95(name)\n\nval spark = SparkSession.\n    builder().\n    getOrCreate()\n", 
            "outputs": [], 
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "// Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n// Please read the documentation of 'SparkSession.read()' and 'DataFrameReader' to learn more about the possibilities to adjust the data loading.\n// Spark documentation: hhttp://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.sql.DataFrameReader@json%28paths:String*%29:org.apache.spark.sql.DataFrame\n\nval ds1 = spark.read.json(\"swift://Databricks.\" + name + \"/iot_devices.json\")\nds1.take(10).foreach(println(_))\n", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "1\n[8,868,US,USA,United States,1,meter-gauge-1xbYRYcj,51,68.161.225.1,38.0,green,-97.0,Celsius,34,1458444054093]\n[7,1473,NO,NOR,Norway,2,sensor-pad-2n2Pea,70,213.161.254.1,62.47,red,6.15,Celsius,11,1458444054119]\n[2,1556,IT,ITA,Italy,3,device-mac-36TWSKiT,44,88.36.5.1,42.83,red,12.83,Celsius,19,1458444054120]\n[6,1080,US,USA,United States,4,sensor-pad-4mzWkz,32,66.39.173.154,44.06,yellow,-121.32,Celsius,28,1458444054121]\n[4,931,PH,PHL,Philippines,5,therm-stick-5gimpUrBB,62,203.82.41.9,14.58,green,120.97,Celsius,25,1458444054122]\n[3,1210,US,USA,United States,6,sensor-pad-6al7RTAobR,51,204.116.105.67,35.93,yellow,-85.46,Celsius,27,1458444054122]\n[3,1129,CN,CHN,China,7,meter-gauge-7GeDoanM,26,220.173.179.1,22.82,yellow,108.32,Celsius,18,1458444054123]\n[0,1536,JP,JPN,Japan,8,sensor-pad-8xUD6pzsQI,35,210.173.177.1,35.69,red,139.69,Celsius,27,1458444054123]\n[3,807,JP,JPN,Japan,9,device-mac-9GcjZ2pw,85,118.23.68.227,35.69,green,139.69,Celsius,13,1458444054124]\n[7,1470,US,USA,United States,10,sensor-pad-10BsywSYUF,56,208.109.163.218,33.61,red,-111.89,Celsius,26,1458444054125]\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Three things happen here under the hood in the code above:\nSpark reads the JSON, infers the schema, and creates a collection of DataFrames. At this point, Spark converts your data into DataFrame = Dataset[Row], a collection of generic Row object, since it does not know the exact type. Now, Spark converts the Dataset[Row] -> Dataset[DeviceIoTData] type-specific Scala JVM object, as dictated by the class DeviceIoTData. Most of us have who work with structured data are accustomed to viewing and processing data in either columnar manner or accessing specific attributes within an object. With Dataset as a collection of Dataset[ElementType] typed objects, you seamlessly get both compile-time safety and custom view for strongly-typed JVM objects. And your resulting strongly-typed Dataset[T] from above code can be easily displayed or processed with high-level methods.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "val ds = spark.read.json(\"swift://Databricks.\" + name + \"/iot_devices.json\").as[DeviceIoTData]\nds.take(10).foreach(println(_))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "DeviceIoTData(1,meter-gauge-1xbYRYcj,68.161.225.1,US,USA,United States,38.0,-97.0,Celsius,34,51,8,868,green,1458444054093)\nDeviceIoTData(2,sensor-pad-2n2Pea,213.161.254.1,NO,NOR,Norway,62.47,6.15,Celsius,11,70,7,1473,red,1458444054119)\nDeviceIoTData(3,device-mac-36TWSKiT,88.36.5.1,IT,ITA,Italy,42.83,12.83,Celsius,19,44,2,1556,red,1458444054120)\nDeviceIoTData(4,sensor-pad-4mzWkz,66.39.173.154,US,USA,United States,44.06,-121.32,Celsius,28,32,6,1080,yellow,1458444054121)\nDeviceIoTData(5,therm-stick-5gimpUrBB,203.82.41.9,PH,PHL,Philippines,14.58,120.97,Celsius,25,62,4,931,green,1458444054122)\nDeviceIoTData(6,sensor-pad-6al7RTAobR,204.116.105.67,US,USA,United States,35.93,-85.46,Celsius,27,51,3,1210,yellow,1458444054122)\nDeviceIoTData(7,meter-gauge-7GeDoanM,220.173.179.1,CN,CHN,China,22.82,108.32,Celsius,18,26,3,1129,yellow,1458444054123)\nDeviceIoTData(8,sensor-pad-8xUD6pzsQI,210.173.177.1,JP,JPN,Japan,35.69,139.69,Celsius,27,35,0,1536,red,1458444054123)\nDeviceIoTData(9,device-mac-9GcjZ2pw,118.23.68.227,JP,JPN,Japan,35.69,139.69,Celsius,13,85,3,807,green,1458444054124)\nDeviceIoTData(10,sensor-pad-10BsywSYUF,208.109.163.218,US,USA,United States,33.61,-111.89,Celsius,26,56,7,1470,red,1458444054125)\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "ds.getClass.getSimpleName", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "Dataset"
                    }, 
                    "execution_count": 5, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "ds.count()", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "198164"
                    }, 
                    "execution_count": 6, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Displaying your Dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Viewing a Dataset\nOnce you have loaded the JSON data and converted into a Dataset for your type-specific collection of JVM objects, you can view them as you would view a DataFrame, by using either display() or using standard Spark commands, such as take(), foreach(), and println() API calls.\n\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "// display the dataset table just read in from the JSON file\n//  display(ds)\nds.show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n|battery_level|c02_level|cca2|cca3|           cn|device_id|         device_name|humidity|             ip|latitude|   lcd|longitude|  scale|temp|    timestamp|\n+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n|            8|      868|  US| USA|United States|        1|meter-gauge-1xbYRYcj|      51|   68.161.225.1|    38.0| green|    -97.0|Celsius|  34|1458444054093|\n|            7|     1473|  NO| NOR|       Norway|        2|   sensor-pad-2n2Pea|      70|  213.161.254.1|   62.47|   red|     6.15|Celsius|  11|1458444054119|\n|            2|     1556|  IT| ITA|        Italy|        3| device-mac-36TWSKiT|      44|      88.36.5.1|   42.83|   red|    12.83|Celsius|  19|1458444054120|\n|            6|     1080|  US| USA|United States|        4|   sensor-pad-4mzWkz|      32|  66.39.173.154|   44.06|yellow|  -121.32|Celsius|  28|1458444054121|\n|            4|      931|  PH| PHL|  Philippines|        5|therm-stick-5gimp...|      62|    203.82.41.9|   14.58| green|   120.97|Celsius|  25|1458444054122|\n|            3|     1210|  US| USA|United States|        6|sensor-pad-6al7RT...|      51| 204.116.105.67|   35.93|yellow|   -85.46|Celsius|  27|1458444054122|\n|            3|     1129|  CN| CHN|        China|        7|meter-gauge-7GeDoanM|      26|  220.173.179.1|   22.82|yellow|   108.32|Celsius|  18|1458444054123|\n|            0|     1536|  JP| JPN|        Japan|        8|sensor-pad-8xUD6p...|      35|  210.173.177.1|   35.69|   red|   139.69|Celsius|  27|1458444054123|\n|            3|      807|  JP| JPN|        Japan|        9| device-mac-9GcjZ2pw|      85|  118.23.68.227|   35.69| green|   139.69|Celsius|  13|1458444054124|\n|            7|     1470|  US| USA|United States|       10|sensor-pad-10Bsyw...|      56|208.109.163.218|   33.61|   red|  -111.89|Celsius|  26|1458444054125|\n|            3|     1544|  IT| ITA|        Italy|       11|meter-gauge-11dlM...|      85|  88.213.191.34|   42.83|   red|    12.83|Celsius|  16|1458444054125|\n|            0|     1260|  US| USA|United States|       12|sensor-pad-12Y2kIm0o|      92|    68.28.91.22|    38.0|yellow|    -97.0|Celsius|  12|1458444054126|\n|            6|     1007|  IN| IND|        India|       13|meter-gauge-13Gro...|      92| 59.144.114.250|    28.6|yellow|     77.2|Celsius|  13|1458444054127|\n|            1|     1346|  NO| NOR|       Norway|       14|sensor-pad-14QL93...|      90| 193.156.90.200|   59.95|yellow|    10.75|Celsius|  16|1458444054127|\n|            9|     1259|  US| USA|United States|       15|  device-mac-15se6mZ|      70|    67.185.72.1|   47.41|yellow|   -122.0|Celsius|  13|1458444054128|\n|            4|     1425|  US| USA|United States|       16|sensor-pad-16aXmI...|      53|   68.85.85.106|    38.0|   red|    -97.0|Celsius|  15|1458444054128|\n|            0|     1466|  US| USA|United States|       17|meter-gauge-17zb8...|      98|161.188.212.254|   39.95|   red|   -75.16|Celsius|  31|1458444054129|\n|            4|     1096|  CN| CHN|        China|       18|sensor-pad-18XULN9Xv|      25|  221.3.128.242|   25.04|yellow|   102.72|Celsius|  31|1458444054130|\n|            9|     1531|  US| USA|United States|       19|meter-gauge-19eg1...|      75| 64.124.180.215|    38.0|   red|    -97.0|Celsius|  29|1458444054130|\n|            7|     1155|  US| USA|United States|       20|sensor-pad-20gFNf...|      33|  66.153.162.66|   33.94|yellow|   -78.92|Celsius|  10|1458444054131|\n+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Iterating, transforming, and filtering Dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Let's iterate over the first 10 entries with the foreach() method and print them", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// Using the standard Spark commands, take() and foreach(), print the first \n// 10 rows of the Datasets.\nds.take(10).foreach(println(_))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "DeviceIoTData(1,meter-gauge-1xbYRYcj,68.161.225.1,US,USA,United States,38.0,-97.0,Celsius,34,51,8,868,green,1458444054093)\nDeviceIoTData(2,sensor-pad-2n2Pea,213.161.254.1,NO,NOR,Norway,62.47,6.15,Celsius,11,70,7,1473,red,1458444054119)\nDeviceIoTData(3,device-mac-36TWSKiT,88.36.5.1,IT,ITA,Italy,42.83,12.83,Celsius,19,44,2,1556,red,1458444054120)\nDeviceIoTData(4,sensor-pad-4mzWkz,66.39.173.154,US,USA,United States,44.06,-121.32,Celsius,28,32,6,1080,yellow,1458444054121)\nDeviceIoTData(5,therm-stick-5gimpUrBB,203.82.41.9,PH,PHL,Philippines,14.58,120.97,Celsius,25,62,4,931,green,1458444054122)\nDeviceIoTData(6,sensor-pad-6al7RTAobR,204.116.105.67,US,USA,United States,35.93,-85.46,Celsius,27,51,3,1210,yellow,1458444054122)\nDeviceIoTData(7,meter-gauge-7GeDoanM,220.173.179.1,CN,CHN,China,22.82,108.32,Celsius,18,26,3,1129,yellow,1458444054123)\nDeviceIoTData(8,sensor-pad-8xUD6pzsQI,210.173.177.1,JP,JPN,Japan,35.69,139.69,Celsius,27,35,0,1536,red,1458444054123)\nDeviceIoTData(9,device-mac-9GcjZ2pw,118.23.68.227,JP,JPN,Japan,35.69,139.69,Celsius,13,85,3,807,green,1458444054124)\nDeviceIoTData(10,sensor-pad-10BsywSYUF,208.109.163.218,US,USA,United States,33.61,-111.89,Celsius,26,56,7,1470,red,1458444054125)\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Ease-of-use of APIs with structure\n\nAlthough structure may limit control in what your Spark program can do with data, it introduces rich semantics and an easy set of domain specific operations that can be expressed as high-level constructs. Most computations, however, can be accomplished with Dataset\u2019s high-level APIs. For example, it\u2019s much simpler to perform agg, select, sum, avg, map, filter, or groupBy operations by accessing a Dataset typed object\u2019s DeviceIoTData than using RDD rows\u2019 data fields.\n\nExpressing your computation in a domain specific API is far simpler and easier than with relation algebra type expressions (in RDDs). For instance, the code below will filter() and  map() create another immutable Dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Like RDD, Dataset has transformations and actions methods. Most importantly are the high-level domain specific operations such as sum(), select(), avg(), join(), and union() that are absent in RDDs. For more information, look at the Scala Dataset API.\n\nLet\u2019s look at a few handy ones in action. In the example below, we use filter(), map(), groupBy(), and avg(), all higher-level methods, to create another Dataset, with only fields that we wish to view. What\u2019s noteworthy is that we access the attributes we want to filter by their names as defined in the case class. That is, we use the dot notation to access individual fields. As such, it makes code easy to read and write.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Processing a Dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// filter out all devices whose temperature exceed 25 degrees and generate \n// another Dataset with three fields that of interest and then display \n// the mapped Dataset\nval dsTemp = ds.filter(d => d.temp > 25).map(d => (d.temp, d.device_name, d.cca3))\n//display(dsTemp)\ndsTemp.take(10)", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "Array((34,meter-gauge-1xbYRYcj,USA), (28,sensor-pad-4mzWkz,USA), (27,sensor-pad-6al7RTAobR,USA), (27,sensor-pad-8xUD6pzsQI,JPN), (26,sensor-pad-10BsywSYUF,USA), (31,meter-gauge-17zb8Fghhl,USA), (31,sensor-pad-18XULN9Xv,CHN), (29,meter-gauge-19eg1BpfCO,USA), (30,device-mac-21sjz5h,AUT), (28,sensor-pad-24PytzD00Cp,CAN))"
                    }, 
                    "execution_count": 9, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "dsTemp.take(10).foreach(println(_))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "(34,meter-gauge-1xbYRYcj,USA)\n(28,sensor-pad-4mzWkz,USA)\n(27,sensor-pad-6al7RTAobR,USA)\n(27,sensor-pad-8xUD6pzsQI,JPN)\n(26,sensor-pad-10BsywSYUF,USA)\n(31,meter-gauge-17zb8Fghhl,USA)\n(31,sensor-pad-18XULN9Xv,CHN)\n(29,meter-gauge-19eg1BpfCO,USA)\n(30,device-mac-21sjz5h,AUT)\n(28,sensor-pad-24PytzD00Cp,CAN)\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Performance and Optimization\n\nAlong with all the above benefits, you cannot overlook the space efficiency and performance gains in using DataFrames and Dataset APIs for two reasons.\n\nFirst, because DataFrame and Dataset APIs are built on top of the Spark SQL engine, it uses Catalyst to generate an optimized logical and physical query plan. Across R, Java, Scala, or Python DataFrame/Dataset APIs, all relation type queries undergo the same code optimizer, providing the space and speed efficiency. Whereas the Dataset[T] typed API is optimized for data engineering tasks, the untyped Dataset[Row] (an alias of DataFrame) is even faster and suitable for interactive analysis.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Second, since Spark as a compiler understands your Dataset type JVM object, it maps your type-specific JVM object to Tungsten\u2019s internal memory representation using Encoders. As a result, Tungsten Encoders can efficiently serialize/deserialize JVM objects as well as generate compact bytecode that can execute at superior speeds.\n\n### When should I use DataFrames or Datasets?\n\nIf you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.\nIf your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.\nIf you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten\u2019s efficient code generation, use Dataset.\nIf you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\nIf you are a R user, use DataFrames.\nIf you are a Python user, use DataFrames and resort back to RDDs if you need more control.\n#### Note that you can always seamlessly interoperate or convert from DataFrame and/or Dataset to an RDD, by simple method call .rdd. For instance,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// select specific fields from the Dataset, apply a predicate\n// using the where() method, convert to an RDD, and show first 10\n// RDD rows\nval deviceEventsDS = ds.select($\"device_name\", $\"cca3\", $\"c02_level\").where($\"c02_level\" > 1300)\n// convert to RDDs and take the first 10 rows\nval eventsRDD = deviceEventsDS.rdd.take(10)\neventsRDD.take(10).foreach(println(_))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "[sensor-pad-2n2Pea,NOR,1473]\n[device-mac-36TWSKiT,ITA,1556]\n[sensor-pad-8xUD6pzsQI,JPN,1536]\n[sensor-pad-10BsywSYUF,USA,1470]\n[meter-gauge-11dlMTZty,ITA,1544]\n[sensor-pad-14QL93sBR0j,NOR,1346]\n[sensor-pad-16aXmIJZtdO,USA,1425]\n[meter-gauge-17zb8Fghhl,USA,1466]\n[meter-gauge-19eg1BpfCO,USA,1531]\n[sensor-pad-22oWV2D,JPN,1522]\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "deviceEventsDS.take(10).foreach(println(_))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "[sensor-pad-2n2Pea,NOR,1473]\n[device-mac-36TWSKiT,ITA,1556]\n[sensor-pad-8xUD6pzsQI,JPN,1536]\n[sensor-pad-10BsywSYUF,USA,1470]\n[meter-gauge-11dlMTZty,ITA,1544]\n[sensor-pad-14QL93sBR0j,NOR,1346]\n[sensor-pad-16aXmIJZtdO,USA,1425]\n[meter-gauge-17zb8Fghhl,USA,1466]\n[meter-gauge-19eg1BpfCO,USA,1531]\n[sensor-pad-22oWV2D,JPN,1522]\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "// Apply higher-level Dataset API methods such as groupBy() and avg().\n// Filter temperatures > 25, along with their corresponding\n// devices' humidity, compute averages, groupBy cca3 country codes,\n// and display the results, using table and bar charts\nval dsAvgTmp = ds.filter(d => {d.temp > 25}).map(d => (d.temp, d.humidity, d.cca3)).groupBy($\"_3\").avg()\n \n// display averages as a table, grouped by the country\n//display(dsAvgTmp)\ndsAvgTmp.take(10).foreach(println(_))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "[PSE,30.88888888888889,62.22222222222222]\n[HTI,30.6,75.0]\n[POL,29.929577464788732,62.045271629778675]\n[LVA,29.721804511278197,63.29323308270677]\n[BRB,29.63157894736842,61.21052631578947]\n[ZMB,30.0,60.0]\n[JAM,30.466666666666665,69.46666666666667]\n[BRA,30.09396551724138,61.126724137931035]\n[ARM,30.09090909090909,58.27272727272727]\n[MOZ,29.8,67.8]\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "https://bzhangusc.wordpress.com/2015/03/29/the-column-class/", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Select individual fields using the Dataset method select() where battery_level is greater than 6. \n#### Note this high-level domain specific language API reads like a SQL query", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// Select individual fields using the Dataset method select()\n// where battery_level is greater than 6. Note this high-level\n// domain specific language API reads like a SQL query\n// display(ds.select($\"battery_level\", $\"c02_level\", $\"device_name\").where($\"battery_level\" > 6).sort($\"c02_level\"))\nds.select($\"battery_level\", $\"c02_level\", $\"device_name\").where($\"battery_level\" > 6).sort($\"c02_level\").take(10).foreach(println(_))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "\r                                                                                \r[7,800,sensor-pad-146902NqACUHQISa]\n[7,800,device-mac-155337F0m78fHJKl]\n[7,800,sensor-pad-144602MYaTPv]\n[7,800,device-mac-141741tnSSlTg]\n[9,800,sensor-pad-142282fDTmdvJ]\n[8,800,meter-gauge-148337o50gjrXGEi]\n[8,800,sensor-pad-151562COzl8oo]\n[9,800,device-mac-154917UU1XP7GTj]\n[7,800,device-mac-156087C4ZyQ1]\n[7,800,device-mac-171549aDMEfCiPH]\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Creating temporary table", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// registering your Dataset as a temporary table to which you can issue SQL queries\nds.createOrReplaceTempView(\"iot_device_data\")", 
            "outputs": [], 
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "val results =  spark.sql(\"select cca3 from iot_device_data\")\nresults.show(5)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+----+\n|cca3|\n+----+\n| USA|\n| NOR|\n| ITA|\n| USA|\n| PHL|\n+----+\nonly showing top 5 rows\n\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "// Having saved the Dataset of DeviceIoTData as a temporary table, you can issue SQL queries to it.\nspark.sql(\"select cca3, count (distinct device_id) as device_id from iot_device_data group by cca3 order by device_id desc limit 100\").show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "\r                                                                                \r+----+---------+\n|cca3|device_id|\n+----+---------+\n| USA|    70405|\n| CHN|    14455|\n| JPN|    12100|\n| KOR|    11879|\n| DEU|     7942|\n| GBR|     6486|\n| CAN|     6041|\n| RUS|     5989|\n| FRA|     5305|\n| BRA|     3224|\n| AUS|     3119|\n| ITA|     2915|\n| SWE|     2880|\n| POL|     2744|\n| NLD|     2488|\n| ESP|     2310|\n| TWN|     2128|\n| IND|     1867|\n| CZE|     1507|\n| NOR|     1487|\n+----+---------+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Saving and Reading from Hive table with SparkSession\n\nNext, we are going to create a Hive table and issue queries against it using SparkSession object as you would with a HiveContext.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Creating Hive Table", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import sys.process._", 
            "outputs": [], 
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "//drop the table if exists to get around existing table error\nspark.sql(\"DROP TABLE IF EXISTS iot_hive_table\")", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[]"
                    }, 
                    "execution_count": 18, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "\"ls /gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/iot_hive_table\" !", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "ls: cannot access /gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/iot_hive_table: No such file or directory\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "\"rm -rf /gpfs/global_fs01/sym_shared/YPProdSpark/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work/spark-warehouse/iot_hive_table\" !", 
            "outputs": [], 
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "#### Query the Hive table with the Spark SQL query", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "//drop the table if exists to get around existing table error\nspark.sql(\"DROP TABLE IF EXISTS iot_hive_table\")\n//save as a hive table\nspark.table(\"iot_device_data\").write.saveAsTable(\"iot_hive_table\")\n//make a similar query against the hive table \nval resultsHiveDF = spark.sql(\"select cca3, count (distinct device_id) as device_id from iot_hive_table group by cca3 order by device_id desc limit 100\")\nresultsHiveDF.show(10)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "\r                                                                                \r+----+---------+\n|cca3|device_id|\n+----+---------+\n| USA|    70405|\n| CHN|    14455|\n| JPN|    12100|\n| KOR|    11879|\n| DEU|     7942|\n| GBR|     6486|\n| CAN|     6041|\n| RUS|     5989|\n| FRA|     5305|\n| BRA|     3224|\n+----+---------+\nonly showing top 10 rows\n\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Working and Accessing Catalog metadata", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "spark.catalog.listTables.show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+---------------+--------+-----------+---------+-----------+\n|           name|database|description|tableType|isTemporary|\n+---------------+--------+-----------+---------+-----------+\n| iot_hive_table| default|       null|  MANAGED|      false|\n|iot_device_data|    null|       null|TEMPORARY|       true|\n+---------------+--------+-----------+---------+-----------+\n\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "spark.catalog.listDatabases.show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+-------+----------------+--------------------+\n|   name|     description|         locationUri|\n+-------+----------------+--------------------+\n|default|default database|file:/gpfs/global...|\n+-------+----------------+--------------------+\n\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }
    ], 
    "metadata": {
        "kernelspec": {
            "name": "scala-spark20", 
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.0"
        }, 
        "language_info": {
            "name": "scala", 
            "version": "2.11.8"
        }
    }
}