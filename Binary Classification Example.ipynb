{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "version": "2.7.11", 
            "file_extension": ".py", 
            "name": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "source": "## Binary Classification Example\n### http://bit.ly/2jWiCQO\n\nIn this notebok, we will build a binary classification application using the MLlib Pipelines API (https://spark.apache.org/docs/latest/ml-pipeline.html). \nThe Pipelines API provides higher-level API built on top of DataFrames for constructing ML pipelines. \n\n#### You can read more about the Pipelines API in the programming guide - http://spark.apache.org/docs/latest/ml-guide.html\n\nBinary Classification is the task of predicting a binary label. \nE.g., is an email spam or not spam? Should I show this ad to this user or not? \nWill it rain tomorrowor not? \nThis notebook demonstrates algorithms for making these types of predictions.\n\n## Dataset Review\nThe Adult dataset we are going to use is publicly available at the UCI Machine Learning Repository. \nThis data derives from census data, and consists of information about 48842 individuals and their annual income. \nWe will use this information to predict if an individual earns >50k a year or <=50K a year. \nThe dataset is rather clean, and consists of both numeric and categorical variables.\n\n## Attribute Information:\n\n- age: continuous\n- workclass: Private,Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n- fnlwgt: continuous\n- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc...\n- education-num: continuous\n- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent...\n- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners...\n- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n- sex: Female, Male\n- capital-gain: continuous\n- capital-loss: continuous\n- hours-per-week: continuous\n- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany...\n- Target/Label: - <=50K, >50K", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "import pixiedust", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.sql.types import *\n\nschema = StructType([\n        StructField(\"age\", DoubleType(), True),\n        StructField(\"workclass\", StringType(), True),\n        StructField(\"fnlwgt\", DoubleType(), True),\n        StructField(\"education\", StringType(), True),\n        StructField(\"education_num\", DoubleType(), True),\n        StructField(\"marital_status\", StringType(), True),\n        StructField(\"occupation\", StringType(), True),\n        StructField(\"relationship\", StringType(), True),\n        StructField(\"race\", StringType(), True),\n        StructField(\"sex\", StringType(), True),\n        StructField(\"capital_gain\", DoubleType(), True),\n        StructField(\"capital_loss\", DoubleType(), True),\n        StructField(\"hours_per_week\", DoubleType(), True),\n        StructField(\"native_country\", StringType(), True),\n        StructField(\"income\", StringType(), True)        \n])", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'cc29768790ec45439a43668592b02f84')\n    hconf.set(prefix + '.username', '')\n    hconf.set(prefix + '.password', '')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name)\n\nspark = SparkSession.builder.getOrCreate()\n\n# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n# The SparkSession object is already initalized for you.\n# The following variable contains the path to your file on your Object Storage.\npath_1 = \"swift://Databricks.\" + name + \"/adult.data\"", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "dataset = (spark.read\n  .schema(schema)\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\n  .option('header', 'true')\n  .load(path_1))\n  \n# dataset.take(5)\ndisplay(dataset)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "dataset.printSchema", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "cols = dataset.columns", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### Preprocess Data\nSince we are going to try algorithms like Logistic Regression, we will have to convert the categorical variables in the dataset into numeric variables. \nThere are 2 ways we can do this.\n\n### Category Indexing.\nThis is basically assigning a numeric value to each category from {0, 1, 2, ...numCategories-1}. \nThis introduces an implicit ordering among your categories, and is more suitable for ordinal variables (eg: Poor: 0, Average: 1, Good: 2)\n\n### One-Hot Encoding http://spark.apache.org/docs/latest/ml-features.html#onehotencoder\nThis converts categories into binary vectors with at most one nonzero value (eg: (Blue: [1, 0]), (Green: [0, 1]), (Red: [0, 0]))\n\nIn this dataset, we have ordinal variables like education (Preschool - Doctorate), and also nominal variables like relationship (Wife, Husband, Own-child, etc). \nFor simplicity\u2019s sake, we will use One-Hot Encoding to convert all categorical variables into binary vectors. \nIt is possible here to improve prediction accuracy by converting each categorical column with an appropriate method.\n\nHere, we will use a combination of StringIndexer (http://spark.apache.org/docs/latest/ml-features.html#stringindexer) and OneHotEncoder to convert the categorical variables. \nThe OneHotEncoder will return a SparseVector.\n\nSince we will have more than 1 stages of feature transformations, we use a Pipeline to tie the stages together. \nThis simplifies our code.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "###One-Hot Encoding\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "The above code basically indexes each categorical column using the StringIndexer, and then converts the indexed categories into one-hot encoded variables. \nThe resulting output has the binary vectors appended to the end of each row.\n\nWe use the StringIndexer again here to encode our labels to label indices.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"income\", outputCol = \"label\")\nstages += [label_stringIdx]", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Next, we will use the VectorAssembler (http://spark.apache.org/docs/latest/ml-features.html#vectorassembler) to combine all the feature columns into a single vector column. \nThis will include both the numeric columns and the one-hot encoded binary vector columns in our dataset.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Transform all features into a vector using VectorAssembler\nnumericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We finally run our stages as a Pipeline. \nThis puts the data through all of the feature transformations we described in a single call.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "# Create a Pipeline. https://spark.apache.org/docs/latest/ml-pipeline.html\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(dataset)\ndataset = pipelineModel.transform(dataset)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = dataset.select(selectedcols)\ndisplay(dataset)\n#dataset.take(5)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint trainingData.count()\nprint testData.count()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### Fit and Evaluate Models\nWe are now ready to try out some of the Binary Classification algorithms available in the Pipelines API.\n\nOut of these algorithms, the below are also capable of supporting multiclass classification with the Python API: \n    - Decision Tree Classifier (http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier)\n    - Random Forest Classifier (http://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier)\n\nThese are the general steps we will take to build our models: \n    - Create initial model using the training set \n    - Tune parameters with a ParamGrid and 5-fold Cross Validation \n    - Evaluate the best model obtained from the Cross Validation using the test set\n\nWe will be using the BinaryClassificationEvaluator to evaluate our models. \n    - The default metric used here is areaUnderROC.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Logistic Regression\nYou can read more about Logistic Regression from the Programming Guide here http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression. \nIn the Pipelines API, we are now able to perform Elastic-Net Regularization with Logistic Regression, as well as other linear methods.\n\nNote: As of Spark 2.0.0, The Python API does not yet support multiclass classification for Logistic Regression, but will be available in future.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "predictions.printSchema()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "# View model's predictions and probabilities of each prediction class\n# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)\n#selected.take(5)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We can make use of the BinaryClassificationEvaluator method to evaluate our model. \nThe Evaluator expects two input columns: (rawPrediction, label).", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Note that the default metric for the BinaryClassificationEvaluator is areaUnderROC", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "evaluator.getMetricName()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "The evaluator currently accepts 2 kinds of metrics - areaUnderROC and areaUnderPR. We can set it to areaUnderPR by using evaluator.setMetricName(\u201careaUnderPR\u201d).\n\nNow we will try tuning the model with the ParamGridBuilder and the CrossValidator.\n\nIf you are unsure what params are available for tuning, you can use explainParams() to print a list of all params and their definitions.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "print lr.explainParams()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "As we indicate 3 values for regParam, 3 values for maxIter, and 2 values for elasticNetParam, this grid will have 3 x 3 x 3 = 27 parameter settings for CrossValidator to choose from. We will create a 5-fold cross validator.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We can also access the model\u2019s feature weights and intercepts easily", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "print 'Model Intercept: ', cvModel.bestModel.intercept", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "# weights = cvModel.bestModel.weights\n# on Spark 2.X weights are available as ceofficients\nweights = cvModel.bestModel.coefficients\nweights = map(lambda w: (float(w),), weights)  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF)\n#weightsDF.take(5)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)\n#selected.take(5)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "# Decision Trees", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "You can read more about Decision Trees in the Spark MLLib Programming Guide here (http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier)\n\nThe Decision Trees algorithm is popular because it handles categorical data and works out of the box with multiclass classification tasks.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We can extract the number of nodes in our decision tree as well as the tree depth of our model.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "print \"numNodes = \", dtModel.numNodes\nprint \"depth = \", dtModel.depth", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Make predictions on test data using the Transformer.transform() method.\npredictions = dtModel.transform(testData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "predictions.printSchema()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)\n#selected.take(5)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We will evaluate our Decision Tree model with BinaryClassificationEvaluator.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Entropy and the Gini coefficient are the supported measures of impurity for Decision Trees. This is Gini by default. Changing this value is simple, model.setImpurity(\"Entropy\").", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "dt.getImpurity()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Now we will try tuning the model with the ParamGridBuilder and the CrossValidator.\n\nAs we indicate 3 values for maxDepth and 3 values for maxBin, this grid will have 3 x 3 = 9 parameter settings for CrossValidator to choose from. We will create a 5-fold CrossValidator.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1,2,6,10])\n             .addGrid(dt.maxBins, [20,40,80])\n             .build())", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# Takes ~5 minutes", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "print \"numNodes = \", cvModel.bestModel.numNodes\nprint \"depth = \", cvModel.bestModel.depth", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "# View Best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)\n#selected.take(5)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "# Random Forest", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Random Forests uses an ensemble of trees to improve model accuracy.\n\nYou can read more about Random Forest from the programming guide here (http://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Make predictions on test data using the Transformer.transform() method.\npredictions = rfModel.transform(testData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "predictions.printSchema()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We will evaluate our Random Forest model with BinaryClassificationEvaluator.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Now we will try tuning the model with the ParamGridBuilder and the CrossValidator.\n\nAs we indicate 3 values for maxDepth, 2 values for maxBin, and 2 values for numTrees, this grid will have 3 x 2 x 2 = 12 parameter settings for CrossValidator to choose from. We will create a 5-fold CrossValidator.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\ncvModel = cv.fit(trainingData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "# View Best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "# Make Predictions", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "As Random Forest gives us the best areaUnderROC value, we will use the bestModel obtained from Random Forest for deployment, and use it to generate predictions on new data. In this example, we will simulate this by generating predictions on the entire dataset.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "bestModel = cvModel.bestModel", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Generate predictions for entire dataset\nfinalPredictions = bestModel.transform(dataset)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# Evaluate best model\nevaluator.evaluate(finalPredictions)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "In this example, we will also look into predictions grouped by age and occupation.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "finalPredictions.createOrReplaceTempView(\"finalPredictions\")", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "#https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html#sql", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "display(spark.sql(\"select * FROM finalPredictions\"))", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "#### In an operational environment, analysts may use a similar machine learning pipeline to obtain predictions on new data, organize it into a table and use it for analysis or lead targeting.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSession.html", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }, 
                "scrolled": true
            }, 
            "source": "display(spark.sql(\"SELECT occupation, prediction, count(*) as count FROM finalPredictions GROUP BY occupation, prediction ORDER BY occupation\"))", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "display(spark.sql(\"SELECT age, prediction, count(*) AS count FROM finalPredictions GROUP BY age, prediction ORDER BY age\"))", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }, 
            "source": "display(sqlContext.sql(\"SELECT age, prediction, count(*) AS count FROM finalPredictions GROUP BY age, prediction ORDER BY age\"))", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "#https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-Catalog.html", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "spark.sql(\"SHOW TABLES\").show()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "spark.catalog.listTables()", 
            "outputs": [], 
            "cell_type": "code"
        }
    ], 
    "nbformat_minor": 0
}