{
    "metadata": {
        "name": "Sample ML / MLPipeline Newsgroup Dataset", 
        "kernelspec": {
            "name": "python2", 
            "display_name": "Python 2 with Spark 1.6", 
            "language": "python"
        }, 
        "notebookId": 3124939199107501, 
        "language_info": {
            "version": "2.7.11", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2"
        }
    }, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Run this code in Databricks community edition, as data files are not available here, also see the video around this notebook below.\n\n### https://www.youtube.com/watch?v=OednhGRp938&feature=youtu.be\n#### Its a good example of building a model around raw text data\n* [Full ML Workflow using Pipelines](https://docs.cloud.databricks.com/docs/latest/sample_applications/07%20Sample%20ML/MLPipeline%20Newsgroup%20Dataset.html)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Build, Inspect, and Tune ML Pipelines\n### *In DSX*\n\nA practical ML pipeline often involves a sequence of \n- data pre-processing, \n- feature extraction, \n- model fitting, \n- and validation stages.\nFor example, classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation.\nThough there are many libraries we can use for each stage, connecting the dots is not as easy as it may look, especially with large-scale datasets.\nMost ML libraries are not designed for distributed computation or they do not provide native support for pipeline creation and tuning.\nUnfortunately, this problem is often ignored in academia, and it has received largely ad-hoc treatment in industry, where development tends to occur in manual one-off pipeline implementations.\n\nIn this notebook, we are going to use a simple text classification problem as an example to build an ML pipeline in Spark MLlib,\ninspect it if it doesn't work as expected, and tune hyperparameters.", 
            "execution_count": null
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Let us first import some packages under `spark.ml`.\nfrom pyspark.ml import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.param import *\nfrom pyspark.ml.tuning import *\nfrom pyspark.ml.evaluation import *", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "###Load \"20 Newsgroups\" dataset.\n\nThe dataset we are going to use is a simplified version of the popular 20 newsgroups dataset,\nwhich is a collection of newsgroup articles labeled across 20 newsgroups.\nThe raw dataset can be obtained at https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups.\nTo simplify the demo, we preprocessed the dataset by mapping the 20 newsgroups into binary labels indicating whether it is related to science or not,\nthen we randomly split the dataset into training and test and save both in Parquet format, the default binary format used by Spark."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "%fs ls /databricks-datasets/news20.binary/data-001", 
            "execution_count": null
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Load the training dataset as a Spark DataFrame and cache it because we are going to access it multiple times.\ntraining = sqlContext.read.parquet(\"/databricks-datasets/news20.binary/data-001/training\").cache()", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Call `display(...)` to show a DataFrame.\nIt also shows the columns, e.g.:\n* topic: topic of the newsgroup,\n* text: raw text of the newsgroup article,\n* label: whether the topic is related to science (1) or not (0)."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "display(training.limit(10))", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "We can explore this dataset more by checking the distribution of topics and visualizing the result."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "display(training.groupBy(\"topic\").count())", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "By default, `display(...)` shows the result in a table.\nIt is simple to visualize the result with builtin charts.\nFor example, select the pie chart from the drop down list at the bottom of the table and set keys to \"topic\" and values to \"count\".\nThen you should see the following pie (or donut) chart:"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "display(training.groupBy(\"topic\").count())", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In this example notebook, we want to build an ML pipeline to predict the binary label, i.e., whether a newsgroup article is related to science or not.\nSo let us take a look at the label distribution.\nThis could be done by a simple `groupBy` method followed by `count`.\nSimilar to the pie chart above, you can visualize the result in a bar chart.\nWe can see the labels are not balanced but not very skewed either."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "display(training.groupBy(\"label\").count())", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Before we start building the pipeline, let us also load the test dataset for evaluation."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "test = sqlContext.read.parquet(\"/databricks-datasets/news20.binary/data-001/test\").cache()", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "###Build an ML pipeline to classify newsgroup articles\n\nAs we mentioned in the introduction, a practical ML pipeline might consist of multiple stages like feature extraction, feature transformation, and model fitting.\nWe consider a very simple pipeline that consists of the following stages:\n\n1. **RegexTokenizer**, which tokenizes each article into a sequence of words with a regex pattern,\n2. **HashingTF**, which maps the word sequences produced by RegexTokenizer to sparse feature vectors using the hashing trick,\n3. **LogisticRegression**, which fits the feature vectors and the labels from the training data to a logistic regression model.\n\n<img src=\"http://spark.apache.org/docs/latest/img/ml-Pipeline.png\" style=\"width: 800px;\"/>"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Constructing a pipeline is done by creating each pipeline stage and configuing its parameters.\ntokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"s+\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\", numFeatures=5000)\nlr = LogisticRegression(maxIter=20, regParam=0.01)\n\n# To create an ML pipeline you concatenate a sequence of stages.\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])", 
            "execution_count": null
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# After we construct this ML pipeline, we can fit it using the training data\n# and obtain a fitted pipeline model that can be used for prediction.\nmodel = pipeline.fit(training)", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Check and evaluate prediction results\n\nAfter we obtain a fitted pipeline model, we want to know how well it performs.\nLet us start with some manual checks by displaying the predicted labels."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# After fitting, making predictions is as simple as calling \"transform\" on the model.\nprediction = model.transform(training)\n# Show the predicted labels along with true labels and raw texts.\ndisplay(prediction.select(\"prediction\", \"label\", \"text\").limit(10))", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The predicted labels look accurate. However, we should evaluate the model quantitatively."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Create an evaluator for binary classification and use area under the ROC curve as the evaluation metric.\nevaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\nevaluator.evaluate(prediction)", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The training metric is nearly perfect, but usually this is an indicator of overfitting.\nSo let's check test metric."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Call \"model.transform\" on test data and then evaluate the result.\nevaluator.evaluate(model.transform(test))", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The test metric is too low.\nIt seems something goes wrong rather than overfitting.\nLet's inspect the pipeline stage by stage."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "##Inspect a pipeline"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Prediction from a pipeline actually contains all intermediate outputs from each stage:\n* \"words\" is from the tokenzier,\n* \"features\" is from the hashing TF transformer,\n* \"prediction\", \"probability\", and \"rawPredictions\" are from the logistic regression model.\n\nWe can see this from the schema of \"prediction\"."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "prediction.printSchema()", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "So let's check all of them by calling \"display\" on \"prediction\" directly.\nFor columns with complex types like \"words\", which contains arrays of strings, you can expand the result by clicking the small caret button on the top left.\nWe can see the tokenized words look quite weird.\nThere must be something wrong with the tokenizer."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "display(prediction.limit(10))", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "To check the tokenizer, we use `explainParams` to show the embedded documentation of its params."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "print tokenizer.explainParams()", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "It also shows the default value (if set) and the current value.\nOh, we forgot the backslash in the regex pattern for the tokernizer, which should be \"\\s+\" but not \"\\s+\".\nLet's correct it."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Set the value of \"pattern\" back to \"\\s+\".\ntokenizer.setPattern(\"\\\\s+\")", 
            "execution_count": null
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Fit the pipeline again.\nmodel = pipeline.fit(training)", 
            "execution_count": null
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Check the predictions and make sure the tokenized words look good.\nprediction = model.transform(training)\ndisplay(prediction.limit(10))", 
            "execution_count": null
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# Check training metric.\nevaluator.evaluate(prediction)", 
            "execution_count": null
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "evaluator.evaluate(model.transform(test))", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "It looks better now.\nBut the model is still overfitted.\nAs in all ML pipelines, we need to tune the hyperparameters to reduce the generalization error."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Tune hyperparameters via cross validation\n\nCross validation (see [Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_&#40;statistics&#41;)) is commonly used for hyperparameter tuning.\nMLlib implements a version called [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_&#40;statistics&#41;#k-fold_cross-validation).\nIt takes a list of hyperparameter combinations and an evaluation metric, then searches for the best hyperparameter combination using cross validation."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "# We generate hyperparameter combinations by taking the cross product of some parameter values we want to try.\n# For simplicity, we try different number of features in hashing TF transformer, and regularization parameters in logistic regression.\nparamGrid = ParamGridBuilder() \\\n  .addGrid(hashingTF.numFeatures, [1000, 10000]) \\\n  .addGrid(lr.regParam, [0.05, 0.2]) \\\n  .build()\n\n# Create a cross validator to tune the pipeline with the generated param grid.\ncv = CrossValidator(estimator=pipeline,\n                    evaluator=evaluator,\n                    estimatorParamMaps=paramGrid,\n                    numFolds=2)", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Fitting a cross validation model is the same as fitting a pipeline.\nIt takes longer (depending on the number of hyperparameter combinations to try) to run due to cross validation."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "cvModel = cv.fit(training)", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Let's check evaluation metrics again."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "evaluator.evaluate(cvModel.transform(training))", 
            "execution_count": null
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "evaluator.evaluate(cvModel.transform(test))", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The test metric got improved.\nFor simplicity, we only tested a small number of hyperparameter combinations.\nWe can improve this further by trying more combinations, possibly with a bigger Spark cluster."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "This is it, and thanks for reading this notebook!\nYou can find more details about ML pipelines in Spark in Joseph's Spark Summit talk embedded below\nor the latest (http://spark.apache.org/docs/latest/ml-guide.html)[MLlib user guide]."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "displayHTML(\"\"\"https://www.youtube.com/embed/OednhGRp938\"\"\")", 
            "execution_count": null
        }
    ], 
    "nbformat_minor": 0, 
    "nbformat": 4
}